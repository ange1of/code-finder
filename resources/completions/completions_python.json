["\"Max value recurrent cell can take before being clipped.\")", "CHECKPOINT_PB_LOAD_NAME = \"checkpoint\"", "\"Execution mode.\")", "tree = tree.insert(8)", "ln += len(self.right)", "initial_centroids,", "rv = flask.send_file(", "j = j.parent", "flags.DEFINE_integer(\"temporal_spike_jitter_width\",", "assert rv.status_code == 206", "if not title_re_match:", "print_results(\"Searching\", test_insert_and_search())", "class MyEncoder(flask.json.JSONEncoder):", "a1_new = self._c", "L2_CON_SCALE = 0.0", "self.gamma = np.float64(gamma)", "print(\"ckpt: \", ckpt)", "self._init = True", "dataset_name = datasets.keys()[0]", "app = StaticFileApp(__name__)", "flask.send_from_directory(\"static\", \"bad\\x00\")", "\"Name of file to keep running log of fit likelihoods, \\", "model = build_model(hps, kind=hps.kind, datasets=datasets)", "running_sum_num, running_sum_denom, running_max_denom = rotate_preds(", "running_sum_denoms_.append(running_sum_denom)", "'%s-%.5d-of-%.5d' % (output_prefix, shard, FLAGS.num_shards))", "flags.DEFINE_integer(\"l2_increase_steps\", L2_INCREASE_STEPS,", "out.confs_probs = [tf.nn.sigmoid(x) for x in out.confs_logits]", "flags.DEFINE_string(\"tpu\", \"\", \"TPU address to connect to.\")", "if i1 == i2:", "crop_value_op = tf.reshape(crop_value_op, shape=[-1, args.arch.value_crop_size,", "if partitions <= 0:", "return self._unbound", "yrange = np.linspace(train_data_y.min(), train_data_y.max(), resolution)", "problem.img_height, problem.img_width,", "model.write_model_runs(datasets, output_fname, push_mean)", "print(\"Done.\")", "yield self.label", "fig, ax = plt.subplots()", "template_rendered.send(app, template=template, context=context)", "uncle.color = 0", "section_title_re = re.compile('\\*\\s\\[(.*)\\]')", "ax1 = plt.subplot2grid((2, 2), (0, 0))", "tree = tree.remove(-12)", "and color(self.sibling.left) == 0", "m.loss_ops += [m.reg_loss_op, m.data_loss_op, m.total_loss_op]", "\"\"\"Code for setting up the network for CMP.", "\"prior_sample\", \"write_model_params\"]:", "typ=args.solver.typ, momentum2=args.solver.momentum2,", "shape=[-1, task_params.img_height,", "tree = RedBlackTree(-1)", "The image data set is expected to reside in JPEG files ends up with '.jpg'.", "k12 = K(i1, i2)", "test_tags = model.predict(test_samples, classify=False)", "rv = render('{{ \"<\\0/script>\"|tojson }}')", "num_maps=len(task_params.map_crop_sizes))", "self._kernel_name = kernel", "flags.DEFINE_float(\"max_grad_norm\", MAX_GRAD_NORM,", "flags.DEFINE_integer(\"eval_steps\", 32, \"Number of eval steps per run.\")", "return datasets", "+ self._b", "self.parent.right = None", "value_intermediate_op = []", "assert get_debug_flag() == debug", "yi = self.tags[index]", "\"{\",", "self.min_node = self.bottom_root", "'image/width': _int64_feature(width),", "d['kl_co_weight'] = flags.kl_co_weight", "del data[data.columns.tolist()[0]]", "flags.DEFINE_string(\"train_file_pattern\", \"\", \"Train file pattern.\")", "occupancy = running_sum_num / tf.maximum(running_sum_denom, 0.001)", "assert get_debug_flag() is None", "except TemplateNotFound:", "flags.DEFINE_boolean(\"do_train_encoder_only\", DO_TRAIN_ENCODER_ONLY,", "fn = lambda ns: running_combine(", "other.left_tree_size = other.left_tree_size * 2 + 1", "with tf.name_scope('inputs'):", "sh = x.get_shape().as_list()", "tree.left.right = RedBlackTree(-5, parent=tree.left)", "incremental_locs_ = tf.unstack(incremental_locs, axis=1, num=num_steps)", "value_iteration_network = cu.value_iteration_network", "if self.bottom_root.val < self.min_node.val:", "a1, a2 = self.alphas[i1].copy(), self.alphas[i2].copy()", "app.register_blueprint(bp)", "assert test_tree_chaining()", "return left + (1 - self.color)", "app.add_url_rule(\"/myview/<int:id>\", methods=[\"GET\"], view_func=myview)", "d['do_train_prior_ar_nvar'] = flags.do_train_prior_ar_nvar", "'image/height': _int64_feature(height),", "num_in_category += 1", "from flask.views import MethodView", "args.solver.sync,", "print(\"Loading lowest validation checkpoint in: \", hps.lfads_save_dir)", "utils.write_data(hp_fname, hps_for_saving, use_json=True)", "d['learning_rate_stop'] = flags.learning_rate_stop", "flags.DEFINE_string(\"output_dist\", OUTPUT_DISTRIBUTION,", "for template in loader.list_templates():", "for i in self._all_samples", "headers={\"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\"},", "assert rv == expected", "((\"../\", \"a/b/c\"), \"../a/b/c\"),", "\"training_loss\": loss,", "((\"/a/b/c\", \"\"), \"/a/b/c/\"),", "print(\"scanning all sample!\")", "link_re = re.compile('\\[(.+)\\]\\((http.*)\\)')", "GEN_CELL_INPUT_WEIGHT_SCALE = 1.0", "elif kind == \"posterior_push_mean\":", "return self.parent and self.parent.right is self", "tree = tree.insert(9)", "a2_new_unc = a2 + (y2 * (e1 - e2)) / eta", "'\"9\": \"foo\",',", "return example", "sys.stdout = sys.__stdout__", "if https not in https_keys:", "d = dict.fromkeys(range(20), \"foo\")", "[running_sum_num, running_sum_denom, running_max_denom],", "\"Test samples' feature length does not equal to that of train samples\"", "doctest.testmod()", "\"\"\"Utilities that interact with cloud service.", "images_reshaped = tf.reshape(imgs,", "d['ext_input_dim'] = flags.ext_input_dim", "OUTPUT_FILENAME_STEM = \"\"", "json = flask.request.get_json()", "yield from self.left.postorder_traverse()", "KL_IC_WEIGHT = 1.0", "elif task_params.input_type == 'analytical_counts':", "section_line_num[category] = line_num", "step, m.input_tensors['step']['step_number'][0,0,0])", "\"L2 regularization cost for the generator only.\")", "init_var = np.sqrt(2.0/(ks**2)/neurons)", "b1_new = np.float64(", "flags.DEFINE_string(\"data_dir\", DATA_DIR, \"Data for training\")", "padded_decode = isinstance(strategy, tf.distribute.experimental.TPUStrategy)", "category = \"\"", "if (not self.min_node.left) and (not self.min_node.parent):", "channels = 3", "import copy", "category = line.split(anchor)[1].strip()", "b = b1_new", "DO_FEED_FACTORS_TO_CONTROLLER = True", "self.bottom_root = self.bottom_root.mergeTrees(self.bottom_root.parent)", "LEARNING_RATE_DECAY_FACTOR = 0.95", "self.right.parent = other", "desc_length = len(segments[index_desc])", "labels = [int(df.loc[fid]['landmark_id']) for fid in file_ids]", "return self.__offset", "@app.route(\"/array\")", "flask.json.JSONDecoder.__init__(self, *args, **kwargs)", "test_insertion_speed()", "self.size = self.size + other.size", "link = title_re_match.group(2)", "h1 * f1", "other.parent = None", "raise ValueError(\"gamma value must greater than 0\")", "train_directory/train-00001-of-00128", "labels = [None] * len(image_paths)", "(\"V\u00f6gel.txt\", \"Vogel.txt\", \"V%C3%B6gel.txt\"),", "from official.utils.misc import distribution_utils", "out.deconv_output = x", "flags.DEFINE_string(\"data_filename_stem\", DATA_FILENAME_STEM,", "assert test_insert_delete()", "self.size -= 1", "self.model = model", "True,", "combined_roots_list = []", "task_params.img_width,", "'\"12\": \"foo\",',", "config.gpu_options.allow_growth = True", "flags.DEFINE_float(\"co_prior_var_scale\", CO_PRIOR_VAR_SCALE,", "data_loss_wt=args.solver.data_loss_wt,", "spacing = np.linspace(0, len(image_paths), FLAGS.num_shards + 1, dtype=np.int)", "+ L * f2", "cp_pb_ln = hps.checkpoint_pb_load_name", "d['cell_weight_scale'] = flags.cell_weight_scale", "assert rv.data == f.read()[4:16]", "segments = line.split('|')[1:-1]", "\"init_checkpoint\", None,", "self.left_tree_size = 0", "for i in range(spacing[shard], spacing[shard + 1]):", "assert rv.direct_passthrough", "DATA_DIR = \"/tmp/rnn_synth_data_v1.0/\"", "if kind in [\"posterior_sample_and_average\", \"posterior_push_mean\",", "\"Number of hidden layers of encoder.\")", "file = opener(app)", "m.lr_op, m.global_step_op, m.train_op, m.should_stop_op, m.optimizer,  m.sync_optimizer = tf_utils.setup_training(", "tree = tree.insert(-12)", "df = pd.read_csv(csv_file)", "for key in datasets:", "for i1 in [i for i in self._all_samples if self._check_obey_kkt(i)]:", "problem.node_ids_dim)))", "rv = client.post(\"/json\", data='\"foo\"', content_type=\"application/x+json\")", "from .globals import _request_ctx_stack", "if cmd is None:", "all_not_obey = True", "initialized model!\")", "if self.min_node.right is None:", "data_dict[k] = None", "if alive == 3:", "if batch_norm_param is not None:", "if self.label is None:", "'image/format': _bytes_feature(image_format.encode('utf-8')),", "kind = FLAGS.kind", "class SmoSVM:", "m.value_intermediate_ops = value_intermediate_ops", "flags.DEFINE_integer(\"num_decoder_layers\", 12,", "f.write(content)", "called = []", "tree = tree.remove(9)", "from absl import app", "elif kind == \"write_model_params\":", "AssertionError(\"Flask(import_name) is importing import_name.\")", "if list(tree.inorder_traverse()) != [-16, 0, 8, 16, 20, 22, 24]:", "(\"index.html\", \"index.html\", False),", "bp.json_decoder = MyDecoder", "response = requests.get(", "if FLAGS.allow_gpu_growth:", "self.bottom_root = i", "\"env, ref_env, debug\",", "and color(self.sibling) == 0", "self._c = np.float64(cost)", "(\"production\", \"production\", False),", "m.loss_ops_names += ['reg_loss', 'data_loss', 'total_loss']", "color(self.parent) == 0", "CELL_WEIGHT_SCALE = 1.0", "from sklearn.datasets import make_blobs, make_circles", "CO_MEAN_CORR_SCALE = 0.0", "return _render(", "flags.DEFINE_integer(\"num_encoder_layers\", 12,", "functions of the red-black tree.", "FLAGS.model_dir)", "\"Variance of ic prior distribution\")", "lambda app: io.BytesIO(b\"Test\"),", "grads = tape.gradient(scaled_loss, tvars)", "if val < self.min_node.val:", "return loader.get_source(environment, template)", "weights_initializer=tf.random_normal_initializer(stddev=init_var))", "self.bottom_root = previous_node", "left_rot.left.right = RedBlackTree(5, parent=left_rot.left)", "d['controller_input_lag'] = flags.controller_input_lag", "d['data_dir'] = flags.data_dir", "__slots__ = (\"val\",)", "'Testing data csv file path.')", "import pytest", "locis = yield from self._choose_a1()", "crop_value_op = value_op[:, remove:-remove, remove:-remove,:]", "if appctx is not None:", "rv = client.get(\"/\", headers={\"Range\": \"bytes=4-1000\"})", "return pformat(", "assert meth is not None, f\"Unimplemented method {request.method!r}\"", "return \"List\"", "y1, y2 = self.tags[i1], self.tags[i2]", "if link in previous_links:", "if cors not in cors_keys:", "plot_heterogeneity(heterogeneity, k)", "datetime.date(1975, 1, 5),", "self.size = 0", "\"/\",", "test_linear_kernel(ax2, cost=500)", "((\"/a/b/c\", \"./\"), \"/a/b/c/.\"),", "BATCH_SIZE = 128", "not callable(getattr(Range, \"to_content_range_header\", None)),", "b = (b1_new + b2_new) / 2.0", "flags.DEFINE_boolean(\"do_train_io_only\", DO_TRAIN_IO_ONLY,", "print(\"Loading checkpoint: \", cp_pb_ln, \", in: \", hps.lfads_save_dir)", "d['max_grad_norm'] = flags.max_grad_norm", "self._gen = gen", "add_error(line_num, \"first character of description is not capitalized\")", "i2 = min(tmp_error_dict, key=lambda index: tmp_error_dict[index])", "value_op = fr_op", "with tf.io.gfile.GFile(filename, 'rb') as f:", "while True:", "tree.insert(-8)", "ctx = _app_ctx_stack.top", "for i in range(1, partitions - 1):", "\\\"cpu:0\\\", \\\"gpu:1\\\", etc)\")", "return True", "assert rv.data == b\"Hello\"", "return \"true\"", "sh = [-1, map_crop_size, map_crop_size, task_params.map_channels]", "import sklearn.datasets as ds", "a1_new = a1 + s * (a2 - a2_new)", "self = view.view_class(*class_args, **class_kwargs)", "print(\"No file passed (file should contain Markdown table syntax)\")", "problem.img_channels)))", "preorder.append((curr_node.val, level))", "sh_before = x.get_shape().as_list()", "usage_doc = \"Usage of script: script_nama <size_of_canvas:int>\"", "add_error(line_num, 'Title should not end with \"... API\". Every entry is an API here!')", "FLAGS.len_passage,", "assert \"Unable to infer MIME-type\" in str(excinfo.value)", "'\"0\": \"foo\",',", "for j, _ in enumerate(row):", "self.params = params", "running_sum_num = tf.unstack(previous_sum_num, axis=1, num=1)[0]", "sh = [-1, map_crop_size, map_crop_size, task_params.goal_channels]", "'image/id': _bytes_feature(file_id.encode('utf-8')),", "print(\"Hello!\\nStart test svm by smo algorithm!\")", "name='weight')", "writer.close()", "if label is not None:", "\"hello\",", "import time", "self.bottom_root.parent", "for i in range(test_tags.shape[0]):", "labels = []", "problem.readout_maps_channels)))", "flags.DEFINE_float(\"ic_prior_var_max\", IC_PRIOR_VAR_MAX,", "rv = client.get(\"/\", headers={\"Range\": \"bytes=-10\"})", "flags.DEFINE_float(\"l2_gen_scale\", L2_GEN_SCALE,", "...", "_build_tfrecord_dataset('train', FLAGS.train_csv_path, FLAGS.train_directory)", "set(m.vision_ops.vars_to_restore))", "with tf.variable_scope('action_pred'):", "other.right = self", "return (data - self._min) / (self._max - self._min)", "if previous_node:", "running_sum_denom = running_sum_denom + confs_probs_[i]", "indent=1,", "flags.DEFINE_integer(\"num_steps_for_gen_ic\", NUM_STEPS_FOR_GEN_IC,", "LEARNING_RATE_INIT = 0.01", "left.parent = parent", "left_rot = RedBlackTree(10)", "d['checkpoint_name'] = flags.checkpoint_name", "test_directory/test-00127-of-00128", "self.color = 0", "tree.right.right = RedBlackTree(20, parent=tree.right)", "self._error = np.zeros(self.length)", "batch_size=FLAGS.eval_batch_size,", "return (r < -tol and alphas[index] < c) or (r > tol and alphas[index] > 0.0)", "self.bottom_root = other.bottom_root", "self.bottom_root = self.bottom_root.left", "_plot_trajectories      = nu.plot_trajectories", "\"Initial autocorrelation of AR(1) priors.\")", "if left != right:", "new_centroids = np.array(new_centroids)", "elif cp_pb_ln == 'checkpoint_lve':", "out.encoder_output = tf.reshape(x, shape=[task_params.batch_size, -1, n_views] + sh_before[1:])", "log_device_placement=False)", "new_node.parent = self.bottom_root", "file.close()", "heap_preOrder = []", "DO_TRAIN_PRIOR_AR_NVAR = True", "plt.rcParams.update({\"font.size\": 16})", "data = data.dropna(axis=0)", "flags.DEFINE_string(", "dataset = input_pipeline.get_input_dataset(", "(\"development\", \"development\", True),", "num_pred=task_params.num_actions,", "new_centroids = []", "initial_centroids = get_initial_centroids(dataset[\"data\"], k, seed=0)", "flags.DEFINE_string('test_directory', '/tmp/', 'Testing data directory.')", "ax1.set_title(\"linear svm,cost:0.1\")", "if boolean_value:", "d['kl_start_step'] = flags.kl_start_step", "name='batch_norm_is_training_op')", "out.reshape_conv_feat = tf.reshape(reshape_conv_feat, shape=[-1, sh[1]*n_views])", "print(\"This should only be a few seconds.\")", "@pytest.mark.parametrize(\"tz\", ((\"UTC\", 0), (\"PST\", -8), (\"KST\", 9)))", "x, outs = deconv(f, batch_norm_is_training_op, wt_decay=wt_decay,", "and (prev_cluster_assignment == cluster_assignment).all()", "for i2 in np.roll(self.unbound, np.random.choice(self.length)):", "raise ValueError(\"Invalid dataset name '%s'.\"%(dataset_name))", "\"Cost of correlation (thru time)in the means of \\", "@app.route(\"/json\", methods=[\"POST\"])", "if problem.outputs.readout_maps:", "+ 1 / 2 * h1 ** 2 * K(i1, i1)", "elif label > self.label:", "rv = client.get(\"/?foo=\uc815\uc0c1\ucc98\ub9ac\".encode(\"euc-kr\"))", "tf.test.main()", "test_samples = np.array([(x, y) for x in xrange for y in yrange]).reshape(", "rv = client.get(url)", "flags.DEFINE_integer(\"ic_enc_dim\", IC_ENC_DIM,", "print(\"scanning non-bound sample!\")", "(\"\", False, False),", "import src.file_utils as fu", "assert lines == sorted_by_str", "if combined_roots_list[i][1] != combined_roots_list[i + 1][1]:", "BaseEnvironment.__init__(self, **options)", "flags.DEFINE_integer(", "combined_roots_list[i + 1][0].left = combined_roots_list[i][0]", "prior_sample, write_model_params\")", "assert rv == test_data", "assert rv == '\"\\\\u0026\"'", "bottom_of_new = self.min_node.right", "if split_maps:", "with tf.name_scope('check_size'):", "\"Number of factors from generator\")", "clip_gradient_norm=args.solver.clip_gradient_norm,", "k_matrix = np.zeros([self.length, self.length])", "neurons=mapper_arch.deconv_neurons,", "return call_func", "incremental_locs_[i], incremental_thetas_[i], map_size,", "if self.right:", "if task_params.input_type == 'vision':", "result.add(template)", "'\"11\": \"foo\",',", "strategy=strategy)", "flags.DEFINE_float(\"learning_rate_init\", LEARNING_RATE_INIT,", "train_data_y = train_data[:, 2]", "self.parent.color = 1", "flags.DEFINE_string(\"feedback_factors_or_rates\", FEEDBACK_FACTORS_OR_RATES,", "for map_crop_size in task_params.map_crop_sizes:", "resize_crop_value_ops.append(previous_value_op)", "previous_node = i.left", "LEARNING_RATE_N_TO_COMPARE = 6", "class DispatchingJinjaLoader(BaseLoader):", "self.left.insert(label)", "parent = self.parent", "if title.upper().endswith(' API'):", "vars_to_optimize = list(set(tf.trainable_variables()) -", "if self._e(i1) >= 0:", "@app.route(\"/\", methods=[\"POST\"])", "self,", "del next_gen_canvas", "sess = tf.Session(config=config)", "raise NotImplementedError()", "self.right = other", "return state", "return (self.gamma * np.inner(v1, v2) + self.coef0) ** self.degree", "test_rbf_kernel(ax4, cost=500)", "flags.DEFINE_float(\"ic_post_var_min\", IC_POST_VAR_MIN,", "for val, floor, ceil in tuples:", "return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))", "cost=cost,", "\" exists?\")", "image/class/label: integer, the landmark_id from the input training csv file.", "if tree != right_rot:", "assert cc.max_age == 3600", "flags.DEFINE_string('test_csv_path', '/tmp/test.csv',", "for s in ['sum_num', 'sum_denom', 'max_denom']:", "min_entries_per_section = 3", "methods = None", "print(\"Found training set with number examples: \", train_total_size)", "if prev_cluster_assignment is not None:", "(image.shape[2]))", "self.alphas[i1], self.alphas[i2] = a1_new, a2_new", "etc (.csv appended)\")", "return flask.Response(flask.stream_with_context(Wrapper(generate())))", "fig.canvas.draw()", "rv = {}", "sections = {}", "self.size = 1", "fr_op, num_iters=args.arch.vin_num_iters,", "running_sum_nums_ = []; running_sum_denoms_ = [];", "if args.arch.vin_num_iters > 0:", "assert rv.data == \"\uc815\uc0c1\ucc98\ub9ac\".encode()", "flags.DEFINE_float(\"gen_cell_rec_weight_scale\", GEN_CELL_REC_WEIGHT_SCALE,", "m.summary_ops = {", "reg_loss_wt=args.solver.reg_loss_wt,", "if expected_flag is None:", "+ 1 / 2 * L ** 2 * K(i2, i2)", "rv = client.get(\"/\", headers={\"Range\": \"bytes=-\"})", "left_rot.right = RedBlackTree(20, parent=left_rot)", "app.json_decoder = MyDecoder", "(\"\", \"production\", False),", "bottom_root=bottom_of_new, min_node=min_of_new, heap_size=size_of_new", "GEN_CELL_REC_WEIGHT_SCALE = 1.0", "fig.show()", "\"opener\",", "b = b2_new", "except AssertionError:", "yield self.app, loader", "\"num_hidden_layers\":", "FLAGS.num_decoder_layers,", "bottom_of_new = bottom_of_new.left", "\"Range\": \"bytes=4-15\",", "'\"3\": \"foo\",',", "if loader is not None:", "m.train_ops['batch_norm_is_training_op'] = batch_norm_is_training_op", "import tfcode.cmp_summary as cmp_s", "tree = tree.insert(-16).insert(16).insert(8).insert(24).insert(20).insert(22)", "image_paths, file_ids, labels = _get_image_files_and_labels(", "l1 * f1", "train_total_size = len(data_dict['train_data'])", "\"Increase weight of kl cost to avoid local minimum.\")", "ctc_labels, merge_dups=False, null_label=9)", "app = flask.Flask(__name__)", "if self.color:", "class X:", ").replace(microsecond=0)", "m.resize_crop_value_ops = resize_crop_value_ops", "weight = tf.ones_like(m.input_tensors['train']['action'], dtype=tf.float32,", "((\"a\", \"b\", \"c\"), \"a/b/c\"),", "fr_v2                   = cu.fr_v2", "auto_norm=False,", "d['con_dim'] = flags.con_dim", "\"Maximum number of articles in NHNet, only used when model_type=nhnet\")", "output_file = os.path.join(", "ctx.app.update_template_context(context)", "fr_ops = []; value_ops = [];", "m.train_ops['step'] = m.action_prob_op", "inputs.append(('gt_dist_to_goal', tf.float32, (problem.batch_size, None, 1)))", "assert rv == '\"\\\\u003c\\\\u0000/script\\\\u003e\"'", "m.value_features_op, neurons=args.arch.pred_neurons,", "eta = k11 + k22 - 2.0 * k12", "tree = tree.insert(10)", "import random", "tf.set_random_seed(args.solver.seed)", "self.right = child.right", "and", "self.tags = train[:, 0]", "self.grandparent.rotate_left()", "loader = self.app.jinja_loader", "_build_tfrecord_dataset('test', FLAGS.test_csv_path, FLAGS.test_directory)", "class MethodViewType(type):", "not_obey = False", "is_training=True,", "confs.append(conf)", "right.parent = parent", "tree = tree.insert(4)", "average (not number of samples to average over).\")", "flags.DEFINE_string('train_directory', '/tmp/', 'Training data directory.')", "inputs.append(('running_'+s+'_{:d}'.format(i), tf.float32,", "attempts = []", "wt_decay=args.solver.wt_decay, name='pred', offset=0,", "if cp_pb_ln == 'checkpoint':", "(\"/a\", \"b\", \"/c\"),", "d['l2_start_step'] = flags.l2_start_step", "m.coverage_ops[i],", "if len(segment) - len(segment.lstrip()) != 1 or len(segment) - len(segment.rstrip()) != 1:", "flags.DEFINE_enum(", "non_idempotent_labels = decode._CodesFromCTC(", "if seed is not None:", "centroids = initial_centroids[:]", "yield flask.request.args[\"name\"]", "options[\"loader\"] = app.create_global_jinja_loader()", "ctx.app,", "if remove > 0:", "self.sibling.color = 0", "print(itr, end=\"\")", "write_model_samples(hps, datasets, hps.output_filename_stem)", "if not output_fname:", "yield flask.session[\"test\"]", "ewma_decay = 0.99 if is_training else 0.0", "if tree.floor(val) != floor or tree.ceil(val) != ceil:", "while bottom_of_new.left:", "flags.DEFINE_string('output_directory', '/tmp/', 'Output data directory.')", "m.action_logits_op, _ = tf_utils.fc_network(", "self.size = self.size - 1 - size_of_new", "assert flask.json.loads(rv.data)[\"x\"] == http_date(d.timetuple())", "return self.left.get_min()", "self._all_samples = list(range(self.length))", "while i or j:", "logging.info(\"Stats:\\n%s\", stats)", "raise ValueError('The parsed image number of dimensions is not 3 but %d' %", "(\"/a\", \"b/../b/../../c\"),", "d['co_prior_var_scale'] = flags.co_prior_var_scale", "if utf8:", "assert rv.last_modified == last_modified", "params = params_dict.override_params_dict(", "return self._error[index]", "rv = client.get(\"/\", headers={\"Range\": \"somethingsomething\"})", "for i in range(len(problem.map_crop_sizes)):", "),", "expected = dt.astimezone(gmt).strftime('\"%a, %d %b %Y %H:%M:%S %Z\"')", "flags.DEFINE_integer(\"learning_rate_n_to_compare\", LEARNING_RATE_N_TO_COMPARE,", "flags.DEFINE_float(\"co_mean_corr_scale\", CO_MEAN_CORR_SCALE,", "next_gen_canvas[r][c] = __judge_point(", "flask.Flask(\"importerror\")", "if self.gamma < 0:", "rv = client.get(\"/\", headers={\"Range\": \"bytes=1000-\"})", "non_null_labels, merge_dups=False, null_label=9)", "out.seek(0)", "K = self._k", "d['gen_cell_input_weight_scale'] = flags.gen_cell_input_weight_scale", "train_step_str = '-0'", "meth = getattr(self, \"get\", None)", "m.readout_maps_loss_op = tf.losses.sigmoid_cross_entropy(", "elif problem.input_type == 'analytical_counts':", "if child is None:", "self.assertEqual(idempotent_labels, non_null_labels)", "add_error(line_num, \"{} is not a valid HTTPS option\".format(https))", "canvas = [[False for i in range(size)] for j in range(size)]", "d['kl_ic_weight'] = flags.kl_ic_weight", "print(f\"\\nall: {test_num}\\nright: {score}\\nfalse: {test_num - score}\")", "data = data.replace({\"M\": np.float64(1), \"B\": np.float64(-1)})", "strategy.num_replicas_in_sync)", "return canvas.tolist()", "if kind == \"train\":", "+ H * f2", "params.override(", "i = i.left", "self.bottom_root.parent = next_node", "bp = flask.Blueprint(\"bp\", __name__)", "image_paths = tf.io.gfile.glob(image_dir + '/*.jpg')", ").num_replicas_in_sync", "tzinfo = FixedOffset(hours=tz[1], name=tz[0])", "'\"values\": {',", "assert contains == debug", "results.append(result)", "occupancys.append(occupancy)", "if not os.path.exists(r\"cancel_data.csv\"):", "loader = blueprint.jinja_loader", "return f\"Get {id:d}\"", "or injected directly into generator?\")", "@pytest.mark.skipif(", "_summary_vis            = cmp_s._summary_vis", "if \"train\" in FLAGS.mode:", "d['max_ckpt_to_keep_lve'] = flags.max_ckpt_to_keep_lve", "d['output_filename_stem'] = flags.output_filename_stem", "- y1 * K(i1, i2) * (a1_new - a1)", "data=flask.json.dumps({\"a\": 1, \"b\": 2}),", "TEMPORAL_SPIKE_JITTER_WIDTH = 0", "from werkzeug.http import parse_cache_control_header", "height = image.shape[0]", "add_error(line_num, \"each segment must start and end with exactly 1 space\")", "assert(args.arch.multi_scale), 'removed support for old single scale code.'", "rv = render('{{ \"<!--<script>\"|tojson }}')", "FLAGS.model_type, params, init_checkpoint=FLAGS.init_checkpoint)", "'\"1\": \"foo\",',", "if s == -1:", "class TestSendfile:", "self.sibling.rotate_right()", "app.debug = True", "self.bottom_root = self.bottom_root.parent", "return m", "for category, entries in sections.items():", "self.min_node = new_node", "if self is None:", "if test_tags[i] == predict[i]:", "\"/bp\",", "elif self.parent.left is self:", "if len(self.unbound) > 0:", "return flask.send_file(", "super(Trainer, self).__init__()", "crop_value_ops.append(crop_value_op)", "left_rot.left.left.right = RedBlackTree(-5, parent=left_rot.left.left)", "assert rv.data == f.read()", "out_all = tf.split(value=x, axis=4, num_or_size_splits=2*num_maps)", "\"Start increasing weight after this many steps.\")", "self._check()", "self._min = np.min(data, axis=0)", "m.total_loss_op,", "io.BytesIO(b\"Test\"),", "from tensorflow.python.platform import app", "yield from self.right.preorder_traverse()", "self._b = b", "len(segments), num_segments))", "value?\")", "conf = tf.reshape(conf, shape=sh)", "a sharded data set consisting of TFRecord files", "if not self.check_coloring():", "allocation_list = [f\"0-{bytes_per_partition}\"]", "sorted_by_str = [", "datasets[k] = clean_data_dict(data_dict)", "if bottom_of_new.val < min_of_new.val:", "if len(sys.argv) != 2:", "flags.DEFINE_boolean(\"do_train_prior_ar_atau\", DO_TRAIN_PRIOR_AR_ATAU,", "return getattr(self._io, name)", "e1, e2 = self._e(i1), self._e(i2)", "Acceptable values: 'factors' or 'rates'.\")", "step = int(args.arch.sample_gt_prob_type.split('_')[1])", "alpha_list=None,", "\"Minimum variance of IC posterior distribution.\")", "flags.DEFINE_boolean(\"do_feed_factors_to_controller\",", "parent.right = right", "return [i for i in range(self.length) if self._is_support(i)]", "assert \"<h1>Hello World!</h1>\" in str(f.read())", "if __name__ == '__main__':", "GEN_DIM = 200", "a_list = [", "assert test_rotations()", "kernel_func,", "d['do_train_io_only'] = flags.do_train_io_only", "image/colorspace: string, specifying the colorspace, always 'RGB'", "'activation_fn':tf.nn.relu,", "x, output_neurons=args.arch.fr_neurons,", "verbose=True,", "class TestStreaming:", "self.size = size_of_new", "return self.left.floor(label)", "k11 = K(i1, i1)", "saver.restore(session, ckpt.model_checkpoint_path)", "if self.left is None:", "if not dataset:", "print(\"all non-bound samples fit the KKT condition!\")", "return self.parent.parent", "for k, data_dict in datasets.items():", "m.train_ops['updated_state'] = updated_state", "@app.route(\"/kw\")", "return self.right.ceil(label)", "assert get_env() == ref_env", "tree.insert(12)", "from sklearn.metrics import pairwise_distances", "and color(self.sibling.right) == 1", "flags.DEFINE_string(\"checkpoint_pb_load_name\", CHECKPOINT_PB_LOAD_NAME,", "self.bottom_root = new_node", "if len(sys.argv) < 2:", "dead = 0", "image/format: string, specifying the format, always 'JPEG'", "trainer = Trainer(model, params)", "d['learning_rate_n_to_compare'] = flags.learning_rate_n_to_compare", "num_changed", "if pt:", "inputs.append(('incremental_locs', tf.float32,", "methods.update(base.methods)", "(problem.batch_size, 1, problem.map_crop_sizes[i],", "wt_decay=wt_decay, neurons=num_neurons, strides=strides,", "if 0.0 < self.alphas[index] < self._c:", "config = tf.ConfigProto(allow_soft_placement=True,", "\"The lr is adaptively reduced, stop training at this value.\")", "rand_indices = np.random.randint(0, n, k)", "if not locis:", "rv = client.post(", "num_in_category = min_entries_per_section + 1", "https://www.kaggle.com/tobwey/landmark-recognition-challenge-image-downloader", "saver = model.lve_saver", "for i in range(len(task_params.readout_maps_crop_sizes))]", "self.samples = self._norm(train[:, 1:]) if self._auto_norm else train[:, 1:]", "random.shuffle(choice)", "fr_intermediate_ops.append(fr_intermediate_op)", "right = RedBlackTree.black_height(self.right)", "state = None", "n = data.shape[0]", "train_set = valid_set = None", "rv = client.get(", "print(f\"Rough Accuracy: {score / test_tags.shape[0]}\")", "return tree == ans", "flags.DEFINE_integer(\"con_dim\", CON_DIM,", "return flask.send_file(\"static/index.html\", conditional=True)", "while i.left:", "assert rv == '\"\\\\u0027\"'", "LEARNING_RATE_STOP = 0.00001", "\"d\": \"t\",", "@pytest.mark.usefixtures(\"req_ctx\")", "for status in i:", "import flask", "rv = flask.send_from_directory(FakePath(\"static\"), FakePath(\"hello.txt\"))", "m.input_tensors['common'], m.input_tensors['step'], m.input_tensors['train'] =      _inputs(task_params)", "num_goals = task_params.num_goals", "val_neurons=args.arch.vin_val_neurons,", "from .signals import before_render_template", "if char in punctuation:", "left_rot.left = RedBlackTree(0, parent=left_rot)", "return min_value", "(problem.batch_size, problem.num_goals, 2)))", "map_crop_size_ops[i+1],", "previous_node = self.min_node.left", "return self.left == other.left and self.right == other.right", "writer.write(example.SerializeToString())", "class DecoderTest(tf.test.TestCase):", "'\"10\": \"foo\",',", "GCP_METADATA_URL, headers=GCP_METADATA_HEADER, timeout=5)", "d['data_filename_stem'] = flags.data_filename_stem", "(image.shape))", "class TestJSON:", "print(\"Save directory %s does not exist, creating it.\" % hps.lfads_save_dir)", "d['do_train_readin'] = flags.do_train_readin", "assert rv.data == b\"Hello World!\"", "left = RedBlackTree.black_height(self.left)", "kernel_func=mykernel,", "model = build_model(hps, kind=\"train\", datasets=datasets)", "return None, None", "args.arch.value_crop_size,", "checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)", "from matplotlib.colors import ListedColormap", "plt.title(f\"Heterogeneity of clustering over time, K={k:d}\")", "(i.left_tree_size == i.parent.left_tree_size) and (not i.parent.parent)", "raise ValueError('length of image_paths, file_ids, labels shoud be the' +", "if reqctx is not None:", "IC_DIM = 64", "(problem.batch_size, 1, None, None, 1)))", "assert rv.data == b\"3\"", "utils.write_data(fname, model_params, compression=None)", "logging.info(\"Training restored from the checkpoints in: %s\",", "attempt = self.right.floor(label)", "DO_CAUSAL_CONTROLLER,", "hp_fname = os.path.join(hps.lfads_save_dir, fname)", "preorder_heap = self.preOrder()", "for url in \"/kw\", \"/dict\":", "index_cors = 4", "wt_decay=args.solver.wt_decay)", "https_keys = ['Yes', 'No']", "return k_matrix", "\"Number of steps per graph-mode loop. Only training step \"", "tree = RedBlackTree(0)", "bytes_per_partition = number_of_bytes // partitions", "not has_encoding(\"euc-kr\"), reason=\"The euc-kr encoding is required.\"", "d['gen_cell_rec_weight_scale'] = flags.gen_cell_rec_weight_scale", "if char.upper() != char:", "args.solver.steps_per_decay,", "L, H = max(0.0, a2 - a1), min(self._c, self._c + a2 - a1)", "return self.parent.left", "INJECT_EXT_INPUT_TO_GEN = False", "with tf.variable_scope('fc'):", "import urllib.request", "\"Number of examples to process for posterior sample and \\", "if methods:", "\"Model type to choose a model configuration.\")", "\"f\": True,", "output_fname = \"model_params\"", "'\"14\": \"foo\",',", "plot_partition_boundary(mysvm, train_data, ax=ax)", "self.bottom_root.left = None", "err = '(L{:03d}) {}'.format(line_num + 1, message)", "inputs.append(('readout_maps_{:d}'.format(i), tf.float32,", "add_etags=False,", "flags.DEFINE_boolean(\"do_train_readin\", DO_TRAIN_READIN, \"Whether to train the \\", "tvars = self.trainable_variables", "neurons = mapper_arch.fc_neurons + [out_neurons]", "with strategy.scope():", "self._error[i1] = 0", "lambda app: open(os.path.join(app.static_folder, \"index.html\")),", "if next_node:", "model.write_model_samples(dataset_name, output_fname)", "collapsed_labels, merge_dups=True, null_label=9)", "for decorator in cls.decorators:", "xrange,", "Reference: Advanced Data Structures, Peter Brass", "MAX_CKPT_TO_KEEP = 5", "predicted_value = (", "with tf.variable_scope('readout_maps_deconv'):", "next_gen_canvas = np.array(create_canvas(canvas.shape[0]))", "with tf.name_scope('concat'):", "assert (", "ewma_decay=ewma_decay)", "f, _ = tf_utils.fc_network(f, neurons=neurons, wt_decay=wt_decay,", "m.input_tensors['step']['incremental_locs'] * task_params.map_scales[i],", "name='fc', offset=0,", "self.sibling.rotate_left()", "import decoder", "Sets up the mapper and the planner.", "if self.is_left() and self.parent.is_right():", "if self._init:", "\"Input scaling for rec weights in generator.\")", "return data_dict", "alpha=0.5,", "decorators = ()", "(problem.batch_size, None, 1)))", "CELL_CLIP_VALUE = 5.0", "m.value_features_op = tf.reshape(m.final_value_op, sh, name='reshape_value_op')", "self.left = left.right", "from tensorflow.python.platform import flags", "task_params.map_channels])", "self.degree = np.float64(degree)", "flags.DEFINE_string(\"kind\", \"train\",", "from official.modeling.hyperparams import params_dict", "uncle = self.parent.sibling", "d['ic_dim'] = flags.ic_dim", "from werkzeug.http import parse_options_header", "assert flask.url_for(\"index\", _external=True) == \"http://localhost/\"", "tf.app.run()", "np.float64(0) < a1_new < self._c", "L2_START_STEP = 0", "return flask.jsonify(a_list)", "flags.DEFINE_integer(\"ci_enc_dim\", CI_ENC_DIM,", "\"Cell hidden size, encoder of h0\")", "if build_kind == \"write_model_params\":", "ckpt = tf.train.get_checkpoint_state(hps.lfads_save_dir,", "with app.open_resource(\"static/index.html\") as f:", "assert rv.data == b\"flask\"", "app.request_class = ModifiedRequest", "with open(os.path.join(app.root_path, \"static/index.html\"), \"rb\") as f:", "self.grandparent._insert_repair()", "plt.show()", "\"Filename stem for data dictionaries.\")", "self.size += 1", "\"g\": False,", "raw_title = [x.strip() for x in line.split('|')[1:-1]][0]", "return self.__name", "ln += len(self.left)", "next_node = self.min_node.parent", "CSV_LOG = \"fitlog\"", "errors = []", "view = decorator(view)", "assert f\"filename={ascii}\" in content_disposition", "if name == 'train':", "if list(tree.inorder_traverse()) != [-8, 0, 4, 8, 10, 11, 12]:", "train_data_x = train_data[:, 1]", "min_of_new = bottom_of_new", "class Trainer(tf.keras.Model):", "c=train_data_tags,", "is_strict=False)", "'\"18\": \"foo\",',", "rv.direct_passthrough = False", "\"\"\"Run NHNet model training and eval.\"\"\"", "readout_maps, probs = readout_general(", "L2_GEN_SCALE = 2000.0", "return \"42\"", "yield from self.right.inorder_traverse()", "m.ego_map_ops[i] = tf.pad(m.ego_map_ops[i], paddings=paddings_op)", "test_directory/test-00001-of-00128", "newHeap = BinomialHeap(", "if desc_length > 100:", "self.min_node = min_of_new", "max_to_keep=10,", "problem.readout_maps_crop_sizes[i],", "\"i\": {\"test\": \"dict\"},", "adam_eps=args.solver.adam_eps)", "MAX_CKPT_TO_KEEP_LVE = 5", "d['ic_enc_dim'] = flags.ic_enc_dim", "return locis", "strides=args.arch.rom_arch.strides,", "auth_keys = ['apiKey', 'OAuth', 'X-Mashape-Key', 'No']", "inputs.append(('incremental_thetas', tf.float32,", "c=train_data_tags[support],", "return d", "f\"{(bytes_per_partition * (partitions - 1)) + 1}-\" f\"{number_of_bytes}\"", "args.solver.initial_learning_rate,", "\"If true, only allocate amount of memory needed for \\", "if self.left is None and self.right is None:", "and self.bottom_root.left_tree_size", "@app.route(\"/add\", methods=[\"POST\"])", "m.train_ops['common'] = [m.input_tensors['common']['orig_maps'],", "rv = flask.json.htmlsafe_dumps(\"</script>\")", "if key in self:", "d['gen_dim'] = flags.gen_dim", "return flask.jsonify(**d)", "KEEP_PROB = 0.95", "This script assumes you have downloaded using the provided script:", "elif name == 'test':", "inputs.append(('node_ids', tf.int32, (problem.batch_size, None,", "timeout=timeout)", "inputs.append(('perturbs', tf.float32, (problem.batch_size, None,", "a1_new = 0", "tree.insert(4)", "incremental_thetas_ = tf.unstack(incremental_thetas, axis=1, num=num_steps)", "\"Input scaling for input weights in generator.\")", "return self.parent or self", "print(\"\\nStart plot,please wait!!!\")", "keys = ['train_truth', 'train_ext_input', 'valid_data',", "if i.val < self.min_node.val:", "rv = client.post(\"/json\", data=None, content_type=\"application/json\")", "f1 = y1 * (e1 + b) - a1 * K(i1, i1) - s * a2 * K(i1, i2)", "self.bottom_root = next_node", "error\")", "if parent.right is self:", "args.solver.momentum,", "print('Processing shard ', shard, ' and writing file ', output_file)", "\"}\",", "rotate_preds            = cu.rotate_preds", "stats = train(params, strategy)", "reason=\"not implemented within werkzeug\",", "sections[category].append(title_re_match.group(1).upper())", "f = out.reshape_conv_feat", "l1 = a1 + s * (a2 - L)", "than, to lower learning rate.\")", "_save_all               = nu.save_all", "EXT_INPUT_DIM = 0", "stats = evaluation.continuous_eval(", "combined_roots_list.append((i, True))", "provide_automatic_options = None", "return heap_preOrder", "self.color = color", "if False:", "self.grandparent.rotate_right()", "end_time = time.time()", "cluster_assignment = np.argmin(distances_from_centroids, axis=1)", "running_sum_num, running_sum_denom, running_max_denom =          tf.cond(is_single_step, lambda: fn(1), lambda: fn(num_steps*num_goals))", "if self.black_height() is None:", "flags.DEFINE_integer(\"max_ckpt_to_keep_lve\", MAX_CKPT_TO_KEEP_LVE,", "return self._get_source_fast(environment, template)", "segments = [seg.strip() for seg in segments]", "bp.json_encoder = MyEncoder", "targets = models.remove_sos_from_seq(inputs[\"target_ids\"],", "m.vision_ops.vars_to_restore)", "\"\"\"", "image/width: integer, image width in pixels", "return node.color", "batch_norm_param=fc_batch_norm_param,", "@bp.route(\"/bp\", methods=[\"POST\"])", "\"Batch size to use during training.\")", "tree.right = RedBlackTree(10, parent=tree)", "DO_TRAIN_READIN = True", "assert flask.url_for(\"myview\", _method=\"POST\") == \"/myview/create\"", "optimizer=opt,", "print(\"Done!\")", "before_render_template.send(app, template=template, context=context)", "assert rv.data == b\"somethingsomething\"[4:16]", "app.add_url_rule(\"/myview/create\", methods=[\"POST\"], view_func=myview)", "if \"methods\" not in d:", "\"L2 regularization cost for the controller only.\")", "with pytest.raises(ValueError) as excinfo:", "right.left = self", "' same. But they are %d, %d, %d, respectively' %", "inputs.append(('action', tf.int32, (problem.batch_size, None, problem.num_actions)))", "app.config[\"DEBUG\"] = debug", "allocation_list.append(", "top_root = top_root.parent", "checkpoint_interval=FLAGS.checkpoint_interval)", "elif args.arch.sample_gt_prob_type.split('_')[0] == 'step':", "if self.app.config[\"EXPLAIN_TEMPLATE_LOADING\"]:", "results.append(1 if result > 0 else -1)", "if not a1_new and not a2_new:", "eval_steps=FLAGS.eval_steps,", "problem.perturbs_dim)))", "train_data.update(step_input_data)", "class View:", "hps.dataset_names.append(key)", "self.alphas[i1] * self.tags[i1] * k(i1, sample)", "self._K_matrix = self._calculate_k_matrix()", "+ s * L * l1 * K(i1, i2)", "_eval_dist              = nu.eval_dist", "\"Reset the learning rate to initial value.\")", "return self._kernel_name", "left.right = self", "d = build_hyperparameter_dict(FLAGS)", "for key in d2.keys():", "n_samples=500, centers=2, n_features=2, random_state=1", "levels=(-1, 0, 1),", "self.bottom_root = bottom_of_new", "m.train_ops['init_state'] = [init_state for _ in updated_state]", "if section_title_re.match(line):", "- y2 * K(i2, i1) * (a2_new - a2)", "previous_links.append(link)", "if not (np.float64(0) < a2_new < self._c) and not (", "add_error(line_num, \"section header ({}) not added as a title link\".format(match.group(1)))", "add_error(line_num, \"section header is not formatted correctly\")", "return centroids", "out.confs_logits = out_all[num_maps:]", "\"a\": 0,", "self.bottom_root = None", "FACTORS_DIM = 50", "d['checkpoint_pb_load_name'] = flags.checkpoint_pb_load_name", "sh = [-1, args.arch.vin_val_neurons*((args.arch.value_crop_size)**2)]", "from sklearn.preprocessing import StandardScaler", "test_dates = (", "if verbose:", "canvas = next_gen_canvas", "flags.DEFINE_integer(\"batch_size\", BATCH_SIZE,", "return model", "elif self.label > label:", "params = models.get_model_params(FLAGS.model_type)", "prev_cluster_assignment = None", "assert rv.data == b\"foo\"", "self.choose_alpha = self._choose_alphas()", "\"Restrict the controller create only causal inferred \\", "import sys, os, numpy as np", "return self.size == 0", "return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))", "d['csv_log'] = flags.csv_log", "return self.right.search(label)", "reshape_conv_feat = slim.flatten(out.conv_feat)", "for i in self._all_samples:", "elif ol > oh + self._eps:", "content_type=\"application/json\",", "+ 1 / 2 * l1 ** 2 * K(i1, i1)", "index_auth = 2", "return obj", "is_training=batch_norm_is_training_op, name='fr',", "alive -= 1", "_write_tfrecord(name, image_paths, file_ids, labels)", "datasets = utils.read_datasets(data_dir, data_filename_stem)", "for i in range(len(combined_roots_list) - 1):", "for srcobj, loader in self._iter_loaders(template):", "canvas = np.array(canvas)", "rv = flask.helpers.make_response(\"Hello\")", "add_error(line_num, 'Title syntax should be \"[TITLE](LINK)\"')", "from six.moves import zip", "loss = transformer_metrics.transformer_loss(logits, targets,", "return self.parent and self.parent.left is self", "m.train_ops['state_names'] = state_names", "data=flask.json.dumps({\"x\": {\"_foo\": 42}}),", "assert options[\"filename\"] == \"index.html\"", "Binomial Heap", "kernel_size, batch_norm_is_training_op, wt_decay):", "d['temporal_spike_jitter_width'] = flags.temporal_spike_jitter_width", "return meth(*args, **kwargs)", "flask.session[\"test\"] = \"flask\"", "m.input_tensors['step']['running_max_denom_{:d}'.format(i)],", "m.sample_action_combine_type = args.arch.action_sample_combine_type", "colors=colors,", "probs = tf.sigmoid(x)", "scope='loss')", "DATA_FILENAME_STEM = \"chaotic_rnn_inputs_g1p5\"", "(\"/a\", \"b\", \"c/../..\"),", "if not len(image_paths) == len(file_ids) == len(labels):", "collapsed_labels = decode._CodesFromCTC(", "assert \"filename*\" not in rv.headers[\"Content-Disposition\"]", "for base in bases:", "((\"/a/b\", \"c/X/..\"), \"/a/b/c\"),", "for index, value in enumerate(tmp_error)", "results = []", "test_directory/test-00000-of-00128", "right_rot.right.right.right = RedBlackTree(20, parent=right_rot.right.right)", "return self.path", "app.use_x_sendfile = True", "if previous_value_op is not None:", "flask.request.get_json()", "if match.group(1) not in title_links:", "import sys", "d['max_ckpt_to_keep'] = flags.max_ckpt_to_keep", "== \"https://localhost/\"", "(\"No\", False, False),", "np.sum(", "tree.insert(11)", "d['_clip_value'] = 80", "import uuid", "fr_op, fr_intermediate_op = fr_v2(", "app.add_url_rule(url, str(i), lambda val=d: flask.jsonify(x=val))", "for i in range(300000):", "new_node = Node(val)", "assert lines == sorted_by_int", "assert False, (\"Kind %s is not implemented. \" % kind)", "self._io = io.BytesIO(*args, **kwargs)", "sess.run(model.learning_rate.initializer)", "if match:", "alive += 1", "m.sample_gt_prob_op = tf_utils.step_gt_prob(", "return 0", "train,", "m.ego_map_ops = []; m.coverage_ops = []", "assert cc.max_age == 12 * 60 * 60", "other.size = self.size", "strategy,", "PRIOR_AR_AUTOCORRELATION = 10.0", "is_training, batch_norm_is_training_op, num_maps,", "self.right = right.left", "if valid_total_size == 0:", "out_neurons = (mapper_arch.fc_out_size**2)*mapper_arch.fc_out_neurons", "category = line.split(' ')[1]", "if record_heterogeneity is not None:", "return self._kernel(v1, v2)", "c = run(c)", "'\"15\": \"foo\",',", "d['do_causal_controller'] = flags.do_causal_controller", "trainer.compile(", "text = decode.StringFromCTC(ctc_labels, merge_dups=True, null_label=9)", "return None", "if sorted(entries) != entries:", "ax4.set_title(\"rbf kernel svm,cost:500\")", "tmp_error_dict = {", "if a1_new > self._c:", "valid_total_size = len(data_dict['valid_data'])", "heterogeneity = []", "return 10", "for blueprint in self.app.iter_blueprints():", "self._remove_repair()", "return stats", "m.train_ops['step_data_cache'] = [m.vision_ops.encoder_output]", "color(self.parent) == 1", "return \"'{} {}'\".format(self.label, (self.color and \"red\") or \"blk\")", "import doctest", "inputs\")", "return left", "args.solver.num_workers,", "\"padded_decode\": padded_decode,", "self.left = None", "flags.DEFINE_string(\"checkpoint_name\", CHECKPOINT_NAME,", "epochs = FLAGS.train_steps // steps_per_epoch", "match = anchor_re.match(line)", "if eta > 0.0:", "train(hps, datasets)", "x = tf.concat(to_concat, 3)", "kernel_size=args.arch.rom_arch.kernel_size,", "from werkzeug.exceptions import BadRequest", "flags.DEFINE_float(\"prior_ar_atau\",  PRIOR_AR_AUTOCORRELATION,", "grid = test_tags.reshape((len(xrange), len(yrange)))", "'\"13\": \"foo\",',", "[\"test list\", 2, False],", "rv = client.get(\"/?name=World\")", "from tensorflow.contrib import slim", "train_data_x,", "return heterogeneity", "for err in errors:", "to_concat = [occupancy, conf, goal]", "k22 = K(i2, i2)", "return running_sum_nums, running_sum_denoms, running_max_denoms", "h1 = a1 + s * (a2 - H)", "content_disposition = rv.headers[\"Content-Disposition\"]", "for i in neighbours:", "if stats:", "CANCER_DATASET_URL = \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"", "s = y1 * y2", "self.parent.left = None", "return datetime.timedelta()", "parent.left = right", "if ckpt:", "add_error(line_num, \"entry does not have all the required sections (have {}, need {})\".format(", "assert \"filename*=UTF-8''\" not in content_disposition", "if not os.path.exists(hps.lfads_save_dir):", "tree = tree.remove(15)", "if train_total_size == 0:", "if ol < (oh - self._eps):", "self.parent.sibling.color = 0", "train_data = np.hstack((train_y.reshape(500, 1), train_x_scaled))", "i = next_node", "np.mat(grid).T,", "self.app = app", "lambda app: open(os.path.join(app.static_folder, \"index.html\"), \"rb\"),", "decode = decoder.Decoder(filename=None)", "title = title_re_match.group(1)", "check_alphabetical(lines)", "add_error(section_line_num[category], \"{} section is not in alphabetical order\".format(category))", "opt = optimizer.create_optimizer(params)", "with tf.variable_scope('scale_{:d}'.format(i)):", "hps.ndatasets = len(hps.dataset_names)", "3.14,", "scaled_loss = loss / self._num_replicas_in_sync", "from src import utils", "app.add_url_rule(url, url, lambda x=test_value: flask.jsonify(x))", "\"for running on TPUs, `mirrored` uses GPUs with single host.\")", "i.left_tree_size == i.parent.left_tree_size", "]", "self._init = False", "\"Feedback the factors or the rates to the controller? \\", "anchor_re = re.compile(anchor + '\\s(.+)')", "if attempt is not None:", "previous_node.parent = next_node", "datetime.datetime(1973, 3, 11, 6, 30, 45),", "trv = None", "'image/channels': _int64_feature(channels),", "if self.alphas[index] > 0:", "flags.DEFINE_boolean(\"do_reset_learning_rate\", DO_RESET_LEARNING_RATE,", "from .debughelpers import explain_template_loading_attempts", "print_results(\"Deleting\", test_insert_delete())", "return _render(ctx.app.jinja_env.from_string(source), context, ctx.app)", "tree = tree.rotate_right()", "\"happens inside the loop.\")", "assert \"x-sendfile\" in rv.headers", "\"Train only the input (readin) and output (readout) \\", "summary_callback = tf.keras.callbacks.TensorBoard(", "for i1 in [", "@app.route(\"/\")", "stats = {}", "m.loss_ops += [m.readout_maps_loss_op]", "right_rot.right.right.left = RedBlackTree(5, parent=right_rot.right.right)", "yield \"!\"", "self.grandparent.color = self.parent.color", "FLAGS.output_directory,", "url = \"/jsonify_basic_types\"", "idempotent_labels = decode._CodesFromCTC(", "os.path.dirname(__file__), \"test_apps\", \"subdomaintestmodule\"", "checkpoint,", "train_x_scaled = scaler.fit_transform(train_x, train_y)", "if isinstance(d2[key], bool):", "app.root_path, \"static/index.html\"", "params,", "file = io.BytesIO(b\"somethingsomething\")", "for i2 in np.roll(self._all_samples, np.random.choice(self.length)):", "from official.utils.misc import keras_utils", "d['l2_increase_steps'] = flags.l2_increase_steps", "if 0.0 < a2_new < self._c:", "\"params_override\",", "KL_START_STEP = 0", "flags.DEFINE_integer(\"factors_dim\", FACTORS_DIM,", "remove = args.arch.crop_remove_each", "_eval_ap                = nu.eval_ap", "assert rv.data == b'\"<42>\"'", "(problem.batch_size, None, 2)))", "tf.local_variables_initializer())", "x=dataset,", "raise ValueError(", "errors.append(err)", "app.json_encoder = MyEncoder", "elif alive > 3:", "if tree != left_rot:", "elif self.is_left():", "self._kernel = self._get_kernel(kernel_name=kernel)", "if self._kernel == self._rbf:", "write_model_parameters(hps, hps.output_filename_stem, datasets)", "if i < len(task_params.map_crop_sizes)-1:", "i, j = self.bottom_root, other.bottom_root", "index: value", "tree.insert(16)", "m.init_fn = slim.assign_from_checkpoint_fn(args.solver.pretrained_path,", "if args.arch.sample_gt_prob_type == 'inverse_sigmoid_decay':", "flags.DEFINE_integer(\"eval_timeout\", 3000, \"Timeout waiting for checkpoints.\")", "if 0.0 < a1_new < self._c:", "self.params.label_smoothing,", "@count_time", "pt, canvas[r - 1 : r + 2, c - 1 : c + 2]", "print(\"Loading latest training checkpoint in: \", hps.lfads_save_dir)", "assert test_insert_and_search()", "split_maps=True):", "kernel_size=mapper_arch.deconv_kernel_size,", "flags.DEFINE_string(\"eval_file_pattern\", \"\", \"Eval file pattern.\")", "IC_POST_VAR_MIN = 0.0001", "import matplotlib.pyplot as plt", "self.params.vocab_size)", "elif self.is_right() and self.parent.is_left():", "for i in range(len(problem.readout_maps_crop_sizes)):", "assert flask.json.JSONEncoder().encode(dt) == expected", "confs_probs_ = tf.unstack(confs_probs, axis=1, num=num_steps)", "confs = []; occupancys = [];", "return centroids, cluster_assignment", "headers={", "test_rbf_kernel(ax3, cost=0.1)", "flags.DEFINE_boolean(\"do_train_prior_ar_nvar\", DO_TRAIN_PRIOR_AR_NVAR,", "previous_value_op = None", "updated_state += [running_sum_num, running_sum_denom, running_max_denom]", "return self", "MAX_GRAD_NORM = 200.0", "d['learning_rate_decay_factor'] = flags.learning_rate_decay_factor", "rv = flask.send_from_directory(\"static\", \"hello.txt\")", "align_corners=True)", "readin matrices and bias vectors. False leaves them fixed \\", "app.add_url_rule(url, url, lambda: flask.jsonify(x=test_uuid))", "],", "if classify:", "if left is None or right is None:", "centroids, cluster_assignment = kmeans(", "assert rv.status_code == 200", "rv = client.get(\"/\")", "flags.DEFINE_integer(\"ic_dim\", IC_DIM, \"Dimension of h0\")", "mysvm.fit()", "return flask.Response(gen())", "for segment in segments:", "FLAGS.num_encoder_layers,", "monkeypatch.setenv(\"FLASK_DEBUG\", debug)", "m.loss_ops = []; m.loss_ops_names = [];", "from tensorflow.contrib.slim import arg_scope", "for key in http_method_funcs:", "next_node.left = self.bottom_root", "return d2", "value_ops.append(value_op)", "if self.min_node.val > other.min_node.val:", "test_demonstration()", "output_valid_mask=False)[0]", "if hps.do_reset_learning_rate:", "from official.nlp.nhnet import evaluation", "inputs = []", "\"Is the value for atau an init, or the constant value?\")", "model_type=FLAGS.model_type,", "x, out.vars_to_restore = get_repr_from_image(", "(\"\u0442\u0435:/\u0441\u0442\", '\":/\"', \"%D1%82%D0%B5%3A%2F%D1%81%D1%82\"),", "rv = client.get(\"/\", headers={\"Range\": \"bytes=4-\"})", "self.left_tree_size = self.left_tree_size * 2 + 1", "self.min_node = previous_node or next_node", "heterogeneity = 0.0", "if problem.input_type == 'vision':", "to_concat.append(previous_value_op)", "self.__name = name", "\"The output directory where the model checkpoints will be written.\")", "flask.send_file(io.BytesIO(b\"LOL\"), attachment_filename=\"filename\")", "stats = dict(training_loss=float(train_hist[\"training_loss\"][-1]))", "rv[\"request\"] = reqctx.request", "model = build_model(hps, kind=\"write_model_params\", datasets=datasets)", "conv_fn=slim.conv2d_transpose, offset=0, name='deconv')", "m.confs = confs", "distances = pairwise_distances(", "if (", "((\"/..\",), \"/..\"),", "explain_template_loading_attempts(self.app, template, attempts)", "self.color = 1", "self.left = other.right", "bottom_of_new.parent = None", "self.right = RedBlackTree(label, 1, self)", "paddings[2:4,:] = args.mapper_arch.pad_map_with_zeros_each[i]", "maps = {\"linear\": self._linear, \"poly\": self._polynomial, \"rbf\": self._rbf}", "\"Number of steps to train the generator initial conditon.\")", "return flask.json.dumps(flask.request.get_json()[\"x\"])", "'Training data csv file path.')", "print_results(\"Floor and ceil\", test_floor_ceil())", "support = model.support", "assert app.config[\"JSON_SORT_KEYS\"]", "request = urllib.request.Request(", "record_heterogeneity=heterogeneity,", "tol = self._tol", "class MethodView(View, metaclass=MethodViewType):", "\"Name of output file (postfix will be added)\")", "for i in range(k):", "ax.cla()", "\"\"\"Converts landmark image data to TFRecords file format with Example protos.", "tree.insert(24)", "mapper_arch.fc_out_size,", "member_data_points = data[cluster_assignment == i]", "OUTPUT_DISTRIBUTION = 'poisson'", "print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)", "affine functions.\")", "data, k, initial_centroids, maxiter=500, record_heterogeneity=None, verbose=False", "called.append(42)", "sys.exit(1)", "num_actions=task_params.num_actions,", "elif args.arch.sample_gt_prob_type == 'zero':", "ans.right.right.left = RedBlackTree(10, 1, ans.right.right)", "for i in range(len(task_params.map_crop_sizes)):", "auto_norm=True,", "tree = tree.insert(11)", "other.left = self.right", "d2[key] = jsonify_bool(d2[key])", "self.right.insert(label)", "sorted_by_int = [", "tree.right.left = RedBlackTree(5, parent=tree.right)", "if k not in data_dict:", "\"Number of control net outputs (>0 builds that graph).\")", "history = trainer.fit(", "self.sibling.color = 1", "d['do_reset_learning_rate'] = flags.do_reset_learning_rate", "if self.left and self.right:", "k = self._k", "tree.left.left = RedBlackTree(-20, parent=tree.left)", "m.readout_maps_logits = tf.reshape(readout_maps, gt_shape)", "(problem.batch_size, None, problem.map_crop_sizes[i],", "self._unbound = []", "L2_INCREASE_STEPS = 2000", "state = True", "'\"19\": \"foo\",',", "continue", "if self.right is None:", "if self.val < other.val:", "if partitions >= number_of_bytes:", "d['ic_prior_var_scale'] = flags.ic_prior_var_scale", "self.parent = right", "running_max_denoms_.append(running_max_denom)", "if args.mapper_arch.pad_map_with_zeros_each[i] > 0:", "[", "image_data = f.read()", "exception = BadRequest", "layers_per_block=layers_per_block, kernel_size=kernel_size,", "import utils", "flags.DEFINE_integer(\"ext_input_dim\", EXT_INPUT_DIM, \"Dimension of external \\", "flags.DEFINE_enum(\"mode\", \"train\", [\"train\", \"eval\", \"train_and_eval\"],", "and color(self.sibling.left) == 1", "CO_PRIOR_VAR_SCALE = 0.1", "if output_prefix == 'test':", "KL_CO_WEIGHT = 1.0", "(\"True\", True, True),", "DO_CAUSAL_CONTROLLER = False", "gt_ego_maps = [m.input_tensors['step']['readout_maps_{:d}'.format(i)]", "self.right.remove(label)", "== self.bottom_root.parent.left_tree_size", "model_params = model.eval_model_parameters(use_nested=False,", "flags.DEFINE_integer(\"checkpoint_interval\", 2000, \"Checkpointing interval.\")", "self.__traversal(curr_node.left, preorder, level + 1)", "with app.test_request_context():", "category, min_entries_per_section, num_in_category))", "init_state = tf.constant(0., dtype=tf.float32, shape=[", "if auth.replace('`', '') not in auth_keys:", "\"\u00d1and\u00fa\uff0fping\u00fcino.txt\",", "\"e\": \"Hi\",", "self.parent = left", "self.optimizer.apply_gradients(list(zip(grads, tvars)))", "if len(errors) > 0:", "except KeyboardInterrupt:", "tree = tree.rotate_left()", "dead += 1", "if trv is None:", "canvas_size = int(sys.argv[1])", "DO_TRAIN_PRIOR_AR_ATAU = True", "(len(image_paths), len(file_ids), len(labels)))", "a2_new = a2", "m.ego_map_ops.append(ego_map_op)", "for k in keys:", "b2_new = np.float64(", "class TestUrlFor:", "tree.insert(8)", "index_https = 3", "if __name__ == \"__main__\":", "r = self._e(index) * self.tags[index]", "write_version=tf.train.SaverDef.V2)", "\"distribution_strategy\", \"mirrored\", [\"tpu\", \"mirrored\"],", "return trv", "ans = RedBlackTree(0, 0)", "mapper_arch.encoder, freeze_conv, wt_decay, is_training)", "KL_INCREASE_STEPS = 2000", "allocation_list.append(length)", "\"Cell hidden size, controller\")", "self.bottom_root.left = new_node", "output_fname = output_fname + \"_model_params\"", "n_views = len(task_params.aux_delta_thetas) + 1", "cls.methods = methods", "\"test_value\", [0, -1, 1, 23, 3.14, \"s\", \"longer string\", True, False, None]", "(problem.batch_size, None, len(problem.aux_delta_thetas)+1,", "goal = tf.reshape(m.input_tensors['step']['ego_goal_imgs_{:d}'.format(i)], shape=sh)", "inputs.append(('orig_maps', tf.float32,", "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):", "inside_neurons=args.arch.fr_inside_neurons,", "size_of_new = 1", "'\"17\": \"foo\",',", "return image_paths, file_ids, labels", "print(\"Writing model parameters to: \", fname)", "(\"/a\", \"../b/c\"),", "model.global_step = opt.iterations", "app.config[\"JSON_AS_ASCII\"] = test_value", "- y1 * K(i1, i1) * (a1_new - a1)", "return next(self._gen)", "fc_batch_norm_param = {'center': True, 'scale': True,", "if all_not_obey:", "m.total_loss_op = m.total_loss_op + m.readout_maps_loss_op", "\"len_passage\":", "m.reg_loss_op, m.data_loss_op, m.total_loss_op, m.acc_ops =    compute_losses_multi_or(m.action_logits_op,", "if line.startswith(anchor):", "m.vision_ops = get_map_from_images(", "f, as_attachment=True, attachment_filename=\"index.html\"", "flags.DEFINE_boolean(\"inject_ext_input_to_gen\",", "assert flask.safe_join(*args) == expected", "return view", "d['output_dist'] = flags.output_dist", "if args.solver.pretrained_path is not None:", "other.min_node = self.min_node", "running_sum_nums = tf.stack(running_sum_nums_, axis=1)", "self.params.pad_token_id)", "tf.reduce_max(ego_map_op, reduction_indices=[4],", "not_obey = True", "build_kind = \"train\"", "m.coverage_ops.append(coverage_op)", "next_node.left = i", "self.left = left", "gx = np.dot(self.alphas * self.tags, self._K_matrix[:, index]) + self._b", "\"\"\"Tests for decoder.\"\"\"", "if args.solver.freeze_conv:", "output_fname = \"model_runs_\" + hps.kind", "import tfcode.nav_utils as nu", "tree.left = RedBlackTree(-10, parent=tree)", "multi_scale_belief = tf.stop_gradient(multi_scale_belief)", "check_format(sys.argv[1])", "state = pt", "from flask.helpers import get_debug_flag", "assert flask.url_for(\"myview\", _method=\"GET\") == \"/myview/\"", "@property", "coverage_op = tf.cast(tf.greater_equal(", "step_input_data, _ = tf_utils.setup_inputs(inputs)", "self._tol = np.float64(tolerance) if tolerance > 0.0001 else np.float64(0.001)", "if other.size == 0:", "import tfcode.cmp_utils as cu", "latest_filename=cp_pb_ln)", "else:", "image/channels: integer, specifying the number of channels, always 3", "if len(image.shape) != 3:", "train_y[train_y == 0] = -1", "m.global_step_op)", "while self.bottom_root.left:", "coverage_op = tf.ones_like(ego_map_op) * coverage_op", "CHECKPOINT_NAME = \"lfads_vae\"", "plt.plot(heterogeneity, linewidth=4)", "\"Cell hidden size, encoder of control inputs\")", "inputs.append(('goal_loc', tf.float32,", "return np.array(results)", "self.left.remove(value)", "mykernel = Kernel(kernel=\"linear\", degree=5, coef0=1, gamma=0.5)", "x, outs = deconv(multi_scale_belief, batch_norm_is_training_op,", "all_occupancys = tf.concat(m.occupancys + m.confs, 3)", "os.path.getmtime(os.path.join(app.root_path, \"static/index.html\"))", "return self._get_source_explained(environment, template)", "summary_dir, update_freq=max(100, FLAGS.steps_per_loop))", "for i in range(len(m.ego_map_ops)):", "title_links = []", "flags.DEFINE_integer(\"eval_batch_size\", 4, \"Total batch size for evaluation.\")", "flags.DEFINE_integer(\"max_ckpt_to_keep\", MAX_CKPT_TO_KEEP,", "\"Learning rate initial value\")", "return self.search(label) is not None", "writer = tf.io.TFRecordWriter(output_file)", "hps.dataset_names = []", "meth = getattr(self, request.method.lower(), None)", "saver = model.seso_saver", "Use objective function check which alpha2 new could get the minimal objectives", "lw=0,", "m.action_prob_op = tf.nn.softmax(m.action_logits_op)", "steps_per_epoch=steps_per_epoch,", "running_max_denoms = tf.stack(running_max_denoms_, axis=1)", "if self.left and not self.left.check_coloring():", "m.sample_gt_prob_op = tf_utils.inverse_sigmoid_decay(args.arch.isd_k,", "return flask.jsonify(values=d)", "self.label = child.label", "pass", "print(\"Created model with fresh parameters.\")", "flags.DEFINE_integer(\"controller_input_lag\", CONTROLLER_INPUT_LAG,", "rv = None", "rv = template.render(context)", "width = image.shape[1]", "content = response.read().decode(\"utf-8\")", "test_samples = self._norm(test_samples)", "self.bottom_root = bottom_root", "if self.isEmpty():", "assert test_insert()", "model = models.create_model(", "get_repr_from_image     = nu.get_repr_from_image", "@pytest.mark.parametrize(\"debug\", (True, False))", "from jinja2 import Environment as BaseEnvironment", "vars_to_optimize = None", "yield from self.right.postorder_traverse()", "self.parent.rotate_right()", "flask.send_file(io.BytesIO(b\"LOL\"))", "i2 = max(tmp_error_dict, key=lambda index: tmp_error_dict[index])", "ax.matshow(c, cmap=cmap)", "rv = flask.json.dumps(\"\\N{SNOWMAN}\")", "class TestNoImports:", "from absl import logging", "mykernel = Kernel(kernel=\"rbf\", degree=5, coef0=1, gamma=0.5)", "view.__doc__ = cls.__doc__", "while i:", "flags.DEFINE_string(\"model_type\", \"nhnet\",", "return self.label", "Session. Otherwise, use full GPU memory.\")", "\"%C3%91and%C3%BA%EF%BC%8Fping%C3%BCino.txt\",", "distribution_strategy=FLAGS.distribution_strategy, tpu_address=FLAGS.tpu)", "for url in \"/args_unpack\", \"/array\":", "assert rv.data.strip() == b\"Hello Subdomain\"", "with pytest.raises(NotFound):", "i = i.mergeTrees(i.parent)", "for test_sample in test_samples:", "\"num_decoder_layers\":", "push_mean=True)", "return self._K_matrix[i1, i2]", "assert called == [42]", "b = self._b", "with tf.device(hps.device):", "@pytest.mark.parametrize(", "flags.DEFINE_integer(\"gen_dim\", GEN_DIM,", "return flask.jsonify(*a_list)", "common_input_data, _ = tf_utils.setup_inputs(inputs)", "\"Max norm of gradient before clipping.\")", "write_model_runs(hps, datasets, hps.output_filename_stem,", "state_names += ['running_sum_num_{:d}'.format(i),", "checkpoint_callback = keras_utils.SimpleCheckpoint(checkpoint_manager)", "rv = render('{{ \"</script>\"|tojson }}')", "incremental_thetas, previous_sum_num, previous_sum_denom,", "callbacks=[summary_callback, checkpoint_callback],", "print(\"Did not load training set.\")", "return f\"<{o.val}>\"", "running_sum_denom = tf.unstack(previous_sum_denom, axis=1, num=1)[0]", "features = {", "((\"a/b/c\", \"X/..\"), \"a/b/c/.\"),", "non_null_labels = decode._CodesFromCTC(", "out = utils.Foo()", "for itr in range(maxiter):", "if self._auto_norm:", "raise ValueError('The parsed image channels is not 3 but %d' %", "return X(obj[\"_foo\"])", "tuples = [(-20, None, -16), (-10, -16, 0), (8, 8, 8), (50, 24, None)]", "\"Should factors[t-1] be input to controller at time t?\")", "return right", "'\"8\": \"foo\",',", "for j in self._all_samples:", "crop_value_ops = [];", "url = f\"/datetest{i}\"", "return", "and color(self.sibling.right) == 0", "elif kind == \"posterior_sample_and_average\":", "}", "char = segments[index_desc][0]", "lambda app: PyBytesIO(b\"Test\"),", "'\"2\": \"foo\",',", "compute_losses_multi_or = nu.compute_losses_multi_or", "assert \"x-sendfile\" not in rv.headers", "ans.right.right = RedBlackTree(11, 0, ans.right)", "num_changed = np.sum(prev_cluster_assignment != cluster_assignment)", "verbose=2)", "model, train_data, ax, resolution=100, colors=(\"b\", \"k\", \"r\")", "if color(self.sibling) == 1:", "if self.right and not self.right.check_coloring():", "layers_per_block=args.arch.rom_arch.layers_per_block,", "from matplotlib import pyplot as plt", "return new_centroids", "if self.size == 1:", "seed(c)", "d['co_dim'] = flags.co_dim", "problem.map_crop_sizes[i], problem.goal_channels)))", "return RedBlackTree(None)", "GCP_METADATA_HEADER = {\"Metadata-Flavor\": \"Google\"}", "assert rv == '\"\\\\u003c!--\\\\u003cscript\\\\u003e\"'", "self.assertEqual(collapsed_labels, [1, 2, 3, 0, 1, 1])", "yield hello", "IC_PRIOR_VAR_MIN = 0.1", "add_error(line_num, \"{} is not a valid CORS option\".format(cors))", "myview = MyView.as_view(\"myview\")", "assert flask.json.loads(rv.data) == d", "methods = set()", "with tf.variable_scope(\"LFADS\", reuse=None):", "(\"0\", False, False),", "if parent is not None:", "punctuation = ['.', '?', '!']", "left_rot.left.left = RedBlackTree(-10, parent=left_rot.left)", "train_data_tags = train_data[:, 0]", "normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_param,", "import requests", "\"b\": 23,", "GCP_METADATA_URL = \"http://metadata/computeMetadata/v1/instance/hostname\"", "record_heterogeneity.append(score)", "include_strs=\"LFADS\")", "size_of_new = size_of_new * 2 + 1", "strategy = distribution_utils.get_distribution_strategy(", "for r, row in enumerate(canvas):", "i.left = previous_node", "fname = 'hyperparameters' + train_step_str + '.txt'", "render = flask.render_template_string", "'\"9\": \"foo\"',", "self.parent.left.color = 1", "self._b = np.float64(b)", "mimetype=\"text/plain\",", "a2_new = L", "if hasattr(cls, key):", "assert test_tree_traversal()", "class Node:", "flags.DEFINE_boolean(\"do_causal_controller\",", "}, is_strict=False)", "(\"/a\", \"..\", \"b/c\"),", "assert rv.data", "self.parent = None", "self.is_right()", "ln = 1", "return False", "lines = [x.strip() for x in rv.data.strip().decode(\"utf-8\").splitlines()]", "os.makedirs(hps.lfads_save_dir)", "return gx - yi", "self.min_node = previous_node", "self._unbound = [i for i in self._all_samples if self._is_unbound(i)]", "'valid_truth', 'valid_ext_input', 'valid_train']", "yield \"Hello \"", "result.update(loader.list_templates())", "d['ci_enc_dim'] = flags.ci_enc_dim", "\"Cell hidden size, generator.\")", "flask.send_file(file, mimetype=\"text/plain\")", "d['ic_post_var_min'] = flags.ic_post_var_min", "super().__init__(name, bases, d)", "dataset = ds.load_iris()", "get_visual_frustum      = cu.get_visual_frustum", "False,", "\"Should observed inputs be input to model via encoders, \\", "assert get_debug_flag() == expected_flag", "test_num = test_tags.shape[0]", "\"len_title\":", "self.__offset = datetime.timedelta(hours=hours)", "assert test_floor_ceil()", "category_line = line_num", "train_data_y,", "df = df.set_index('id')", "train_data_y[support],", "index_link = 5", "inputs.append(('ego_goal_imgs_{:d}'.format(i), tf.float32,", "ax2 = plt.subplot2grid((2, 2), (0, 1))", "\"Start increasing l2 weight after this many steps.\")", "occupancy = tf.reshape(occupancy, shape=sh)", "print(\"Property 5\")", "\"passage_list\":", "hps.num_steps_for_gen_ic = hps.num_steps", "example = _convert_to_example(file_ids[i], image_buffer, height, width,", "methods.add(key.upper())", "self.parent.right.color = 1", "+ y2 * (a2_new - a2) * K(i2, s)", "i = i.parent", "elif self.label < label:", "i1, i2 = self.choose_alpha.send(state)", "test_tags, test_samples = test_data[:, 0], test_data[:, 1:]", "app.config[\"TRAP_BAD_REQUEST_ERRORS\"] = False", "import logging", "class FakePath:", "app.config[\"SEND_FILE_MAX_AGE_DEFAULT\"] = 3600", "reqctx = _request_ctx_stack.top", "mapper_arch.fc_out_neurons], name='re_fc')", "auth = segments[index_auth]", "NUM_STEPS_FOR_GEN_IC = MAX_INT", "rv = app.send_static_file(FakePath(\"index.html\"))", "exception = NotFound", "assert rv_x == str(test_uuid)", "last_modified=last_modified,", "timeout = 0 if FLAGS.mode == \"train_and_eval\" else FLAGS.eval_timeout", "m.final_value_op = crop_value_op", "from .globals import request", "child = self.left or self.right", "TAG = \"K-MEANS-CLUST/ \"", "class RedBlackTree:", "rv = render('{{ \"\\'\"|tojson }}')", "maxiter=400,", "{", "self._error[i2] = 0", "mysvm = SmoSVM(", "+ 1 / 2 * H ** 2 * K(i2, i2)", "all_not_obey = False", "elif alive == 2 or alive == 3:", "if not line.startswith('|') or line.startswith('|---'):", "previous_max_denom, map_size, num_steps):", "print(", "\"c\": 3.14,", "self.alphas = alpha_list if alpha_list is not None else np.zeros(train.shape[0])", "if isinstance(i2, np.ndarray):", "b=0.0,", "_vis                    = cmp_s._vis", "rv = flask.helpers.make_response()", "ans.right.right.right = RedBlackTree(12, 1, ans.right.right)", "out = io.StringIO()", "plt.figure(figsize=(7, 4))", "return a1_new, a2_new", "last_modified = datetime.datetime.utcfromtimestamp(", "main()", "check_entry(line_num, segments)", "import tensorflow as tf", "m.input_tensors['step']['incremental_thetas'],", "tree.insert(-16)", "and i.left_tree_size != i.parent.parent.left_tree_size", "if cls.decorators:", "experimental_steps_per_execution=FLAGS.steps_per_loop)", "add_error(line_num, \"{} is not a valid Auth option\".format(auth))", "value = self.left.get_max()", "d['do_feed_factors_to_controller'] = flags.do_feed_factors_to_controller", "i.parent = next_node", "for shard in range(FLAGS.num_shards):", "m.init_op = tf.group(tf.global_variables_initializer(),", "alpha_list=al,", "d['num_steps_for_gen_ic'] = flags.num_steps_for_gen_ic", "print(\"Did not load validation set.\")", "if image.shape[2] != 3:", "assert rv.mimetype == \"text/html\"", "task_params.batch_size, 1, map_crop_size, map_crop_size,", "from lfads import LFADS", "if other.right:", "import re", "layers_per_block=mapper_arch.deconv_layers_per_block,", "args.solver.max_steps,", "from pprint import pformat", "if kind in [\"train\", \"posterior_sample_and_average\", \"posterior_push_mean\",", "app.add_url_rule(\"/myview/\", methods=[\"GET\"], view_func=myview)", "self.label = label", "if not tree.check_color_properties():", "for line_num, line in enumerate(lines):", "view.methods = cls.methods", "fr_intermediate_ops = []; value_intermediate_ops = [];", "m.readout_maps_loss_op = 10.*m.readout_maps_loss_op", "assert rv_uuid == test_uuid", "raise ValueError('Unsupported dataset split name: %s' % name)", "ctc_labels, merge_dups=True, null_label=9)", "return rv", "self.grandparent.color = 1", "tree.insert(10)", "if curr_node:", "return flask.Response(generate(\"Hello \"))", "member_data_points = data[cluster_assignment == i, :]", "elif a2_new_unc <= L:", "linewidths=(1, 1, 1),", "with tf.io.gfile.GFile(csv_path, 'rb') as csv_file:", "d['do_train_prior_ar_atau'] = flags.do_train_prior_ar_atau", "m.input_tensors['step']['running_sum_denom_{:d}'.format(i)],", "if self._is_unbound(i1):", "d['kl_increase_steps'] = flags.kl_increase_steps", "m.fr_ops = fr_ops", "heterogeneity += np.sum(squared_distances)", "category_line = 0", "map_crop_size = task_params.map_crop_sizes[i]", "if not (11 in tree and 12 in tree and -8 in tree and 0 in tree):", "centroid = member_data_points.mean(axis=0)", "[\"get\", \"post\", \"head\", \"options\", \"delete\", \"put\", \"trace\", \"patch\"]", "return self.right.get_max()", "fname = os.path.join(hps.lfads_save_dir, output_fname)", "inputs.append(('loc_on_map', tf.float32, (problem.batch_size, None, 2)))", "((\"/\", \"a/\", \"b/\", \"c/\"), \"/a/b/c\"),", "keep_dims=True), 1), tf.float32)", "raise ValueError(\"partitions must be a positive number!\")", "cmd = yield i1, i2", "from werkzeug.exceptions import NotFound", "rv_x = flask.json.loads(rv.data)[\"x\"]", "DO_RESET_LEARNING_RATE = False", "samples = np.array(data)[:, :]", "app.open_resource(\"static/index.html\", mode)", "modules_tmpdir.join(\"importerror.py\").write(\"raise NotImplementedError()\")", "al = np.zeros(train_data.shape[0])", "image/height: integer, image height in pixels", "flags.DEFINE_float(\"learning_rate_decay_factor\", LEARNING_RATE_DECAY_FACTOR,", "train_step_str = re.search('-[0-9]+$', ckpt.model_checkpoint_path).group()", "scaler = StandardScaler()", "self.assertEqual(non_idempotent_labels, [1, 2, 3, 0, 1])", "controller output.\")", "hps.kind = kind", "DO_TRAIN_IO_ONLY = False", "train_directory/train-00127-of-00128", "map_crop_size, ns)", "(problem.batch_size, None,", "if self.parent is None:", "section_line_num = {}", "task_params = args.navtask.task_params", "except requests.exceptions.RequestException:", "CO_DIM = 1", "flags.DEFINE_float(\"l2_con_scale\", L2_CON_SCALE,", "step_counter=model.global_step,", "yield from self.left.inorder_traverse()", "m.crop_value_ops = crop_value_ops", "return predicted_value", "ax3.set_title(\"rbf kernel svm,cost:0.1\")", "e.g. '97c0a12e07ae8dd5' or '650c989dd3493748'", "data = pd.read_csv(r\"cancel_data.csv\", header=None)", "matrices and vectors.\")", "raise Exception(usage_doc)", "PS_NEXAMPLES_TO_PROCESS = MAX_INT", "class PyBytesIO:", "d = {}", "with tf.GradientTape() as tape:", "assert rv.mimetype == \"text/plain\"", "sections[category] = []", "DO_TRAIN_ENCODER_ONLY = False", "labels[i])", "'\"4\": \"foo\",',", "return \"Create\"", "last_modified = datetime.datetime(1999, 1, 1)", "args.solver.learning_rate_decay,", "PRIOR_AR_PROCESS_VAR = 0.1", "if len(obj) == 1 and \"_foo\" in obj:", "assert rv.status_code == 400", "return self.Kernel(self.samples[i1], i2)", "if mapper_arch.dim_reduce_neurons > 0:", "\"static/index.html\", as_attachment=True, attachment_filename=filename", "if self.left:", "m.input_tensors['step']['running_sum_num_{:d}'.format(i)],", "flags.DEFINE_integer(\"len_passage\", 200, \"Passage length.\")", "fss_logits_ = tf.unstack(fss_logits, axis=1, num=num_steps)", "fr_ops.append(fr_op)", "ans.right = RedBlackTree(8, 1, ans)", "np.random.seed(seed)", "print(\"Are you sure you sure a checkpoint in \", hps.lfads_save_dir,", "from .signals import template_rendered", "decode = decoder.Decoder(filename=_testdata('charset_size_10.txt'))", "rv = flask.send_file(\"static/index.html\")", "m.readout_maps_gt = tf.concat(gt_ego_maps, 4)", "assert rv.headers[\"x-sendfile\"] == os.path.join(", "trv = rv", "flags.DEFINE_float(\"ic_prior_var_min\", IC_PRIOR_VAR_MIN,", "m.sample_gt_prob_op = tf.constant(-1.0, dtype=tf.float32)", "except LookupError:", "parent.right = left", "map_crop_size_ops.append(tf.constant(map_crop_size, dtype=tf.int32, shape=(2,)))", "_vis_readout_maps       = cmp_s._vis_readout_maps", "self.parent.color = 0", "with tf.name_scope('combine_{:d}'.format(num_steps)):", "\"Variance of control input prior distribution.\")", "raise TemplateNotFound(template)", "kernel_size=args.arch.vin_ks, share_wts=args.arch.vin_share_wts,", "return flask.Response(flask.stream_with_context(generate()))", "for i1 in self._all_samples", "padding='SAME', scope='dim_reduce',", "return ln", "print_results(\"Tree traversal\", test_tree_chaining())", "from official.nlp.nhnet import models", "if color(self.left) == 1 or color(self.right) == 1:", "dt = datetime.datetime(2017, 1, 1, 12, 34, 56, tzinfo=tzinfo)", "train_x, train_y = make_circles(", "print(\"Property 2\")", "previous_node.parent = i", "y1 * (a1_new - a1) * K(i1, s)", "_save_d_at_t            = nu.save_d_at_t", "if self.label == other.label:", "m.readout_maps_probs = tf.reshape(probs, gt_shape)", "'\"Nandu/pinguino.txt\"',", "rv = flask.json.load(out)", "add_error(line_num, \"description should not exceed 100 characters (currently {})\".format(desc_length))", "url_charset = \"euc-kr\"", "other.right.parent = self", "flags.DEFINE_integer(\"l2_start_step\", L2_START_STEP,", "import io", "epochs=epochs,", "score = compute_heterogeneity(data, k, centroids, cluster_assignment)", "FLAGS.len_title,", "previous_links = []", "ax4 = plt.subplot2grid((2, 2), (1, 1))", "]:", "at their initial values specified by the alignment \\", "deconv                  = cu.deconv", "((\"a/b\", \"X/../c\"), \"a/b/c\"),", "pytest.raises(ValueError, flask.url_for, \"index\", _scheme=\"https\")", "CI_ENC_DIM = 128", "\"Train only the encoder weights.\")", "\"Time lag on the encoding to controller t-lag for \\", "'\"6\": \"foo\",',", "running_max_denoms_ = [];", "prev_cluster_assignment is not None", "cors = segments[index_cors]", "summary_mode: _add_summaries(m, args, summary_mode,", "task_params, args.solver.freeze_conv,", "DO_FEED_FACTORS_TO_CONTROLLER,", "\"Name of checkpoint files (.ckpt appended)\")", "m.coverage_ops[i] = tf.pad(m.coverage_ops[i], paddings=paddings_op)", "m.saver_op = tf.train.Saver(keep_checkpoint_every_n_hours=4,", "image/filename: string, the unique id of the image file", "ans.left = RedBlackTree(-8, 0, ans)", "contains the following fields:", "m.fr_intermediate_ops = fr_intermediate_ops", "cmap = ListedColormap([\"w\", \"k\"])", "d['batch_size'] = flags.batch_size", "\"Initial noise variance for AR(1) priors.\")", "flags.DEFINE_integer('num_shards', 128, 'Number of shards in output data.')", "resolution * resolution, 2", "with open(r\"cancel_data.csv\", \"w\") as f:", "if tree.get_max() != 22 or tree.get_min() != -16:", "ctc_labels = [9, 6, 9, 1, 3, 9, 4, 9, 5, 5, 9, 5, 0, 2, 1, 3, 9, 4, 9]", "self.right = None", "assert rv.data == f.read()[-10:]", "right_rot.right = RedBlackTree(0, parent=right_rot)", "c = self._c", "self._max = np.max(data, axis=0)", "args.solver.adjust_lr_sync,", "as_attachment=True,", "except NotImplementedError:", "\"steps_per_loop\", 1000,", "break", "CONTROLLER_INPUT_LAG = 1", "@app.route(\"/dict\")", "model.train_model(datasets)", "@classmethod", "f = tf.reshape(f, shape=[-1, mapper_arch.fc_out_size,", "n_samples=500, noise=0.1, factor=0.1, random_state=1", "from flask.helpers import get_env", "tree.insert(22)", "params, FLAGS.params_override, is_strict=True)", "d['do_train_encoder_only'] = flags.do_train_encoder_only", "d['cell_clip_value'] = flags.cell_clip_value", "ks = 1; neurons = mapper_arch.dim_reduce_neurons;", "while (", "\"model_dir\", None,", "class Environment(BaseEnvironment):", "directory=FLAGS.model_dir,", "return flask.jsonify(d)", "yield blueprint, loader", "b_old = self._b", "with tf.name_scope('readout_maps'):", "m.input_tensors['train']['action'], weights=weight,", "m.input_tensors['common']['goal_loc']]", "assert rv == '\"\\\\u003c/script\\\\u003e\"'", "((\"a/b/c\",), \"a/b/c\"),", "for i, row in enumerate(canvas):", "images_reshaped, task_params.modalities, task_params.data_augment,", "d2 = d.copy()", "FLAGS.train_batch_size,", "weights_regularizer=slim.l2_regularizer(wt_decay),", "if node is None:", "self.left = RedBlackTree(label, 1, self)", "% (self.label, (self.color and \"red\") or \"blk\"): (self.left, self.right)", "flags.DEFINE_integer(\"kl_increase_steps\", KL_INCREASE_STEPS,", "name, csv_path, image_dir)", "train_hist = history.history", "assert rv.data == f.read()[4:]", "_add_summaries          = cmp_s._add_summaries", "-e2", "from .globals import _app_ctx_stack", "TEMPORAL_SPIKE_JITTER_WIDTH,", "self.left = child.left", "\"<a ng-data='{{ data|tojson }}'></a>\", data={\"x\": [\"foo\", \"bar\", \"baz'\"]}", "(", "alive = 0", "action_neurons=args.arch.vin_action_neurons,", "with open(filename) as fp:", "flags.DEFINE_integer(\"len_title\", 15, \"Title length.\")", "tree.insert(20)", "view.__module__ = cls.__module__", "import os", "logits, _, _ = self(inputs, mode=\"train\", training=True)", "from tfcode import tf_utils", "next_node.left = previous_node", "flags.DEFINE_integer(\"co_dim\", CO_DIM,", "print(err)", "combined_roots_list.append((j, False))", "tree = tree.insert(i)", "self.min_node = None", "k = 3", "rv_uuid = uuid.UUID(rv_x)", "return self[key]", "rv = render(", "if list(tree.preorder_traverse()) != [0, -16, 16, 8, 22, 20, 24]:", "train_data, _ = tf_utils.setup_inputs(inputs)", "member_data_points, [centroids[i]], metric=\"euclidean\"", "if auth != 'No' and (not auth.startswith('`') or not auth.endswith('`')):", "ax3 = plt.subplot2grid((2, 2), (1, 0))", "view.__name__ = name", "\"Distribution Strategy type to use for training. `tpu` uses TPUStrategy \"", "result = set()", "\"Is the value for noise variance an init, or the constant \\", "self.label = value", "ans.right.left = RedBlackTree(4, 0, ans.right)", "):", "if i and ((not j) or i.left_tree_size < j.left_tree_size):", "running_sum_nums_.append(running_sum_num)", "with pytest.raises(exception):", "response = urllib.request.urlopen(request)", "batch_norm_param = mapper_arch.batch_norm_param", "right_rot.left = RedBlackTree(-20, parent=right_rot)", "train_data, test_data = samples[:328, :], samples[328:, :]", "class StaticFileApp(flask.Flask):", "cors_keys = ['Yes', 'No', 'Unknown']", "inputs.append(('step_number', tf.int32, (1, None, 1)))", "rv = flask.send_file(file, mimetype=\"text/plain\")", "d['factors_dim'] = flags.factors_dim", "'running_max_denom_{:d}'.format(i)]", "value, options = parse_options_header(rv.headers[\"Content-Disposition\"])", "lines = list(line.rstrip() for line in fp)", "posterior_sample_and_average, \\", "print(\"Found validation set with number examples: \", valid_total_size)", "args.arch.vin_val_neurons])", "m.occupancys = occupancys", "predict = mysvm.predict(test_samples)", "canvas[i][j] = bool(random.getrandbits(1))", "conv_fn=slim.conv2d_transpose, offset=0,", "assert rv.mimetype == \"application/json\"", "if id is None:", "i = self.bottom_root.parent", "running_sum_denoms = tf.stack(running_sum_denoms_, axis=1)", "print(str(msg), \"works!\" if passes else \"doesn't work :(\")", "self.sibling.right.color = 1", "left = self.left", "DEVICE = \"gpu:0\"", "is_training=is_training,", "'\"5\": \"foo\",',", "previous_value_op = tf.image.resize_bilinear(crop_value_op,", "self._num_replicas_in_sync = tf.distribute.get_strategy(", "if self.size == 0:", "elif color(self.parent) == 0:", "from __future__ import division", "(\"False\", False, False),", "out.conv_feat = slim.conv2d(x, neurons, kernel_size=ks, stride=1,", "if hps.num_steps_for_gen_ic > hps.num_steps:", "k,", "checkpoint_manager = tf.train.CheckpointManager(", "right_rot.right.left = RedBlackTree(-5, parent=right_rot.right)", "if s == i1 or s == i2:", "\"Shuffle spikes around this window.\")", "test_data = {\"name\": \"Flask\"}", "a1_new, a2_new = self._get_new_alpha(*args)", "image = tf.io.decode_jpeg(image_data, channels=3)", "add_error(category_line, \"{} section does not have the minimum {} entries (only has {})\".format(", "for i, d in enumerate(test_dates):", "'\"19\": \"foo\"',", "train_data.update(common_input_data)", "{\"test\": \"dict\"},", "stats = run()", "@pytest.mark.parametrize(\"mode\", (\"w\", \"x\", \"a\", \"r+\"))", "Code for testing the various", "batch_norm_is_training_op=batch_norm_is_training_op,", "dead -= 1", "import pandas as pd", "'image/colorspace': _bytes_feature(colorspace.encode('utf-8')),", "where we have selected 128 shards for both data sets. Each record", "output_fname = output_fname + \"model_runs_\" + hps.kind", "num_in_category = 0", "return \"false\"", "d['prior_ar_atau'] = flags.prior_ar_atau", "\"test_value,expected\", [(True, '\"\\\\u2603\"'), (False, '\"\\u2603\"')]", "rv.close()", "linestyles=(\"--\", \"-\", \"--\"),", "flags.DEFINE_float(\"gen_cell_input_weight_scale\", GEN_CELL_INPUT_WEIGHT_SCALE,", "\"num_nhnet_articles\", 5,", "assert f\"filename*=UTF-8''{utf8}\" in content_disposition", "try:", "\"t\",", "INJECT_EXT_INPUT_TO_GEN,", "url = \"/uuid_test\"", "eval_file_pattern=FLAGS.eval_file_pattern,", "flags.DEFINE_string(\"csv_log\", CSV_LOG,", "m.input_tensors = {}", "class ModifiedRequest(flask.Request):", "self.size = other.size", "tf.global_variables_initializer().run()", "length = f\"{bytes_per_partition * i + 1}-{bytes_per_partition * (i + 1)}\"", "return response.status_code == 200", "return str(json[\"a\"] + json[\"b\"])", "except StopIteration:", "flags.DEFINE_float(\"learning_rate_stop\", LEARNING_RATE_STOP,", "model = LFADS(hps, kind=build_kind, datasets=datasets)", "rv[\"g\"] = appctx.g", "return self.dispatch_request(*args, **kwargs)", "often.\")", "flags.DEFINE_float(\"kl_ic_weight\", KL_IC_WEIGHT,", "if self.label == label:", "flags.DEFINE_float(\"ic_prior_var_scale\", IC_PRIOR_VAR_SCALE,", "return common_input_data, step_input_data, train_data", "self.right = right", "if trv is not None:", "gmt = FixedOffset(hours=0, name=\"GMT\")", "if title_re_match:", "m.ego_map_ops[i],", "if L == H:", "x = tf.reshape(out.encoder_output, shape=[-1] + sh_before[1:])", "train=train_data,", "self.parent.rotate_left()", "rv = render('{{ \"&\"|tojson }}')", "yrange,", "rv = client.post(\"/json\", data=\"malformed\", content_type=\"application/json\")", "while i.parent:", "if a2_new_unc >= H:", "'is_training': batch_norm_is_training_op}", "flask.url_for(\"index\", _external=True, _scheme=\"https\")", "test_uuid = uuid.UUID(bytes=b\"\\xDE\\xAD\\xBE\\xEF\" * 4)", "new_centroids.append(centroid)", "from werkzeug.http import http_date", "flags.DEFINE_string('train_csv_path', '/tmp/train.csv',", "((\"/a\", \"b/c\"), \"/a/b/c\"),", "-e1", "assert flask.json.loads(rv.data) == a_list", "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")", "name='vin', wt_decay=args.solver.wt_decay)", "tree = tree.insert(-8)", "if meth is None and request.method == \"HEAD\":", "assert options[\"filename\"] == \"index.txt\"", "42,", "value_op, value_intermediate_op = value_iteration_network(", "out.fss_logits = out_all[:num_maps]", "self.min_node = min_node", "crop_value_op = value_op", "self.size = heap_size", "import argparse, pprint", "IC_PRIOR_VAR_SCALE = 0.1", "self.left.parent = self", "if self._is_unbound(index)", "rv = app.send_static_file(\"index.html\")", "num_steps = task_params.num_steps", "problem.map_crop_sizes[i], problem.map_channels)))", "print(\"Optimization done!\\nEvery sample satisfy the KKT condition!\")", "result = self._predict(test_sample)", "'image/encoded': _bytes_feature(image_buffer)", "inputs.append(('analytical_counts_{:d}'.format(i), tf.float32,", "ax.contour(", "top_root = self.bottom_root", "next_node = self.bottom_root.parent.parent", "https = segments[index_https]", "from typing import List", "build_kind = kind", "print(flask.safe_join(*args))", "self.assertEqual(text, 'farm barn')", "tolerance=0.001,", "with tf.name_scope('sigmoid'):", "file, attachment_filename=\"filename\", conditional=True", "self.__traversal(top_root, heap_preOrder)", "flags.DEFINE_integer(\"ps_nexamples_to_process\", PS_NEXAMPLES_TO_PROCESS,", "name='readout_maps_deconv')", "if self.is_left():", "return np.exp(-1 * (self.gamma * np.linalg.norm(v1 - v2) ** 2))", "combined_roots_list[i][0].parent = combined_roots_list[i + 1][0]", "[chr(ord(\"b\") + i) for i in range(FLAGS.num_nhnet_articles)],", "dataset[\"data\"],", "m.value_ops = value_ops", "vars_to_optimize=vars_to_optimize,", "add_error(line_num, \"auth value is not enclosed with `backticks`\")", "self.Kernel(self.samples[i, :], self.samples[j, :])", "if self._is_unbound(index):", "return x, probs", "dropout_ratio=mapper_arch.fc_dropout)", "tmp_error = self._error.copy().tolist()", "- y2 * K(i2, i2) * (a2_new - a2)", "\"Learning rate decay, decay by this fraction every so \\", "from __future__ import absolute_import", "return {", "m.loss_ops_names += ['readout_maps_loss']", "\"Strength of KL weight on initial conditions KL penatly.\")", "with tf.variable_scope('deconv'):", "right = self.right", "d['learning_rate_init'] = flags.learning_rate_init", "d['device'] = flags.device", "raise ValueError(\"partitions can not >= number_of_bytes!\")", "class MyDecoder(flask.json.JSONDecoder):", "self.path = path", "choice = [0] * 100 + [1] * 10", "raise ValueError('Kind {} is not supported.'.format(kind))", "return allocation_list", "x = tf.reshape(x, shape=[task_params.batch_size, -1] + sh[1:])", "return list(result)", "help=(\"a YAML/JSON string or a YAML file which specifies additional \"", "hps = hps_dict_to_obj(d)", "assert flask.url_for(\"myview\", id=42, _method=\"GET\") == \"/myview/42\"", "next = __next__", "+ s * H * h1 * K(i1, i2)", "batch_norm_param = args.arch.pred_batch_norm_param", "return attempt", "from official.nlp.nhnet import optimizer", "print(\"Plot done!!!\")", "cp_pb_ln = 'checkpoint' if cp_pb_ln == \"\" else cp_pb_ln", "},", "d['l2_gen_scale'] = flags.l2_gen_scale", "return out", "alphas = self.alphas", "f2 = y2 * (e2 + b) - a2 * K(i2, i2) - s * a1 * K(i1, i2)", "\"/\", headers={\"Range\": \"bytes=4-15\", \"If-Range\": http_date(last_modified)}", "self.bottom_root = Node(val)", "CON_DIM = 128", "@app.route(\"/args_unpack\")", "contains = b\"Failed to decode JSON object\" in rv.data", "tf.reshape(m.readout_maps_gt, [-1, len(task_params.readout_maps_crop_sizes)]),", "hps.dataset_dims[key] = datasets[key]['data_dim']", "flags.DEFINE_string(\"output_filename_stem\", OUTPUT_FILENAME_STEM,", "push_mean=False)", "return 1", "\"Number of previous costs current cost has to be worse \\", "rv = client.get(\"/\", headers={\"Range\": \"bytes=4-15\"})", "args = (i1, i2, a1, a2, e1, e2, y1, y2)", "class FixedOffset(datetime.tzinfo):", "self.coef0 = np.float64(coef0)", "a2_new = a2_new_unc", "from werkzeug.datastructures import Range", "file_ids = [os.path.basename(os.path.normpath(f))[:-4] for f in image_paths]", "args.summary.arop_full_summary_iters)}", "posterior_push_mean, \\", "return flask.request.args[\"foo\"]", "if num_in_category < min_entries_per_section:", "from absl import flags", "\"Maximum variance of IC prior distribution.\")", "return os.path.join('../testdata/', filename)", "\"h\": [\"test list\", 10, False],", "for c, pt in enumerate(row):", "FLAGS.train_file_pattern,", "m.init_fn = None", "return flask.jsonify(foo=str(flask.request.get_json()))", "return flask.request.get_json()", "class hps_dict_to_obj(dict):", "steps_per_epoch = min(FLAGS.train_steps, FLAGS.checkpoint_interval)", "\"Type of model to build {train, \\", "start_time = time.time()", "batch_norm_param['is_training'] = batch_norm_is_training_op", "flags.DEFINE_float(\"kl_co_weight\", KL_CO_WEIGHT,", "_summary_readout_maps   = cmp_s._summary_readout_maps", "m.coverage_ops = m.vision_ops.confs_probs", "rv = flask.send_file(\"static/index.html\", as_attachment=True)", "return other", "if list(tree.postorder_traverse()) != [-16, 8, 20, 24, 22, 16, 0]:", "from jinja2 import TemplateNotFound", "print_results(\"Inserting\", test_insert())", "\"Type of output distribution, 'poisson' or 'gaussian'\")", "@flask.stream_with_context", "i = combined_roots_list[0][0]", "print_results(\"Tree traversal\", test_tree_traversal())", "\"    {:5d} elements changed their cluster assignment.\".format(", "args.solver.wt_decay, is_training, batch_norm_is_training_op,", "for i in range(num_steps):", "(\"/a\", \"b/../../c\"),", "MAX_INT = sys.maxsize", "tree = tree.insert(12)", "\"learning_rate\": self.optimizer._decayed_lr(var_dtype=tf.float32)", "return cluster_assignment", "map_crop_size_ops = []", "is_single_step = tf.equal(tf.unstack(tf.shape(m.ego_map_ops[0]), num=5)[1], 1)", "io.BytesIO(b\"party like it's\"),", "models (rolling)\")", "return self.min_node.val", "app.root_path = os.path.join(", "self.parent._remove_repair()", "squared_distances = distances ** 2", ")", "while top_root.parent:", "d['prior_ar_nvar'] =  flags.prior_ar_nvar", "if 5 in tree or -6 in tree or -10 in tree or 13 in tree:", "if len(segments) < num_segments:", "self.min_node = i", "if not_obey:", "self.assertEqual(non_null_labels, [1, 2, 2, 3, 0, 0, 1, 1])", "return image_data, height, width", "ax2.set_title(\"linear svm,cost:500\")", "wt_decay=args.solver.wt_decay, stride=args.arch.fr_stride)", "cmap=plt.cm.Dark2,", "for s in self.unbound:", "from __future__ import print_function", "FLAGS = flags.FLAGS", "view.provide_automatic_options = cls.provide_automatic_options", "add_error(line_num, \"description should not end with {}\".format(char))", "flags = tf.app.flags", "num_segments = 5", "d['co_mean_corr_scale'] = flags.co_mean_corr_scale", "updated_state = []; state_names = [];", "inputs?\")", "rv[\"session\"] = reqctx.session", "colorspace = 'RGB'", "m.sample_action_type = args.arch.action_sample_type", "c = create_canvas(canvas_size)", "(\"filename\", \"ascii\", \"utf8\"),", "flask.json.dump(test_data, out)", "assert cc.max_age == 10", "state = False", "LFADS_SAVE_DIR = \"/tmp/lfads_chaotic_rnn_inputs_g1p5/\"", "summary_dir = os.path.join(FLAGS.model_dir, \"summaries\")", "CANCER_DATASET_URL,", "d['inject_ext_input_to_gen'] = flags.inject_ext_input_to_gen", "flags.DEFINE_integer(\"kl_start_step\", KL_START_STEP,", "self.__traversal(curr_node.right, preorder, level + 1)", "return self.left.search(label)", "yield from self._choose_a2(i1)", "self.parent = parent", "raw_title = segments[index_title]", "train_directory/train-00000-of-00128", "char = segments[index_desc][-1]", "\"%s %s\"", "class BinomialHeap:", "flags.DEFINE_float(\"prior_ar_nvar\", PRIOR_AR_PROCESS_VAR,", "d['ps_nexamples_to_process'] = flags.ps_nexamples_to_process", "m.input_tensors['step']['imgs'], args.mapper_arch,", "failing = (", "example = tf.train.Example(features=tf.train.Features(feature=features))", "assert rv.status_code == 416", "paddings_op = tf.constant(paddings, dtype=tf.int32)", "assert \"no filename is available\" in str(excinfo.value)", "yield from self.left.preorder_traverse()", "d = {", "cc = parse_cache_control_header(rv.headers[\"Cache-Control\"])", "score += 1", "appctx = _app_ctx_stack.top", "model_dir=FLAGS.model_dir,", "parent.left = left", "params.override({", "running_sum_num = running_sum_num + fss_logits_[i] * confs_probs_[i]", "if sys.version_info >= (3, 8):", "d['ic_prior_var_max'] = flags.ic_prior_var_max", "print(\"Reading data from \", data_dir)", "attachment_filename=\"index.txt\",", "return canvas", "running_max_denom = tf.unstack(previous_max_denom, axis=1, num=1)[0]", "plt.ylabel(\"Heterogeneity\")", "return self.model(inputs, mode)", "self.val = val", "cost=0.4,", "task_params.img_channels], name='re_image')", "tree = tree.insert(15)", "running_max_denom = tf.maximum(running_max_denom, confs_probs_[i])", "print(\"Testing tree balancing...\")", "+ (self._b - b_old)", "'\"16\": \"foo\",',", "add_error(line_num, 'Duplicate link - entries should only be included in one section')", "if dataset_name not in datasets.keys():", "forward, t+lag for reverse.\")", "test_linear_kernel(ax1, cost=0.1)", "assert value == \"attachment\"", "resize_crop_value_ops = [];", "flags.DEFINE_float(\"keep_prob\", KEEP_PROB, \"Dropout keep probability.\")", "from official.nlp.nhnet import input_pipeline", "func(*args, **kwargs)", "title_links.append(section_title_re.match(line).group(1))", "if color(uncle) == 0:", "\"Name of checkpoint files, use 'checkpoint_lve' for best \\", "args.solver.task,", "if \"eval\" in FLAGS.mode:", "self.is_left()", "xrange = np.linspace(train_data_x.min(), train_data_x.max(), resolution)", "m.train_ops['step_data_cache'] = []", "\"If-Range\": http_date(datetime.datetime(1999, 1, 1)),", "flags.DEFINE_float(\"cell_weight_scale\", CELL_WEIGHT_SCALE,", "rv = loader.get_source(environment, template)", "class MyView(MethodView):", "if checkpoint_manager.restore_or_initialize():", "app.run(main)", "return self.parent.right", "self.min_node = other.min_node", "i", "self.Kernel = kernel_func", "test_cancel_data()", "app.url_map.charset = \"euc-kr\"", "http_method_funcs = frozenset(", "flags.DEFINE_float(\"cell_clip_value\", CELL_CLIP_VALUE,", "return pairwise_distances(X, centroids, metric=\"euclidean\")", "return \"\\n\".join((\"-\" * level + str(value)) for value, level in preorder_heap)", "FEEDBACK_FACTORS_OR_RATES = \"factors\"", "image_buffer, height, width = _process_image(image_paths[i])", "left_rot.left.left.left = RedBlackTree(-20, parent=left_rot.left.left)", "\"Number of hidden layers of decoder.\")", "print(f\"smo algorithm cost {end_time - start_time} seconds\")", "'running_sum_denom_{:d}'.format(i),", "assert self.left_tree_size == other.left_tree_size, \"Unequal Sizes of Blocks\"", "assert flask.json.loads(rv.data) == test_value", "if strategy:", "right_rot.right.right = RedBlackTree(10, parent=right_rot.right)", "class TestSafeJoin:", "if a1_new < 0:", "inputs.append(('imgs', tf.float32,", "for args, expected in passing:", "m.train_ops = {}", "if self._check_obey_kkt(i) and self._is_unbound(i)", "self.left._insert_repair()", "if test_samples.shape[1] > self.samples.shape[1]:", "d['keep_prob'] = flags.keep_prob", "flags.DEFINE_boolean(\"allow_gpu_growth\", False,", "0,", "rv = flask.send_file(FakePath(\"static/index.html\"))", "ol = (", "right_rot = RedBlackTree(-10)", "with pytest.raises(ValueError):", "for _srcobj, loader in self._iter_loaders(template):", "return \"\"", "import numpy as np", "within the TFRecord file is a serialized Example proto. The Example proto", "self.right.parent = self", "if not dataset_name:", "import codecs", "logging.info(\"***** Number of cores used : %d\",", "print(\"Possible error!!! You are running \", kind, \" on a newly \\", "if status:", "conf = running_max_denom", "distances_from_centroids = centroid_pairwise_dist(data, centroids)", "train_x, train_y = make_blobs(", "centroids = data[rand_indices, :]", "oh = (", "self.left.remove(label)", "class Kernel:", "from jinja2 import BaseLoader", "return maps[kernel_name]", "k_matrix[i, j] = np.float64(", "features['image/class/label'] = _int64_feature(label)", "cluster_assignment = assign_clusters(data, centroids)", "a2_new = H", "d['l2_con_scale'] = flags.l2_con_scale", "ego_map_op = m.input_tensors['step']['analytical_counts_{:d}'.format(i)]", "return np.inner(v1, v2) + self.coef0", "passing = (", "if member_data_points.shape[0] > 0:", "prev_cluster_assignment = cluster_assignment[:]", "if \"loader\" not in options:", "score = 0", "min_value = self.min_node.val", "elif kind == \"prior_sample\":", "self.sibling.left.color = 1", "ctx.app.jinja_env.get_or_select_template(template_name_or_list),", "d['lfads_save_dir'] = flags.lfads_save_dir", "from official.nlp.transformer import metrics as transformer_metrics", "index_title = 0", "strides=mapper_arch.deconv_strides,", "paddings = np.zeros((5,2), dtype=np.int32)", "if getattr(base, \"methods\", None):", "print(\"all samples fit the KKT condition! Optimization done!\")", "flags.DEFINE_string(\"lfads_save_dir\", LFADS_SAVE_DIR, \"model save dir\")", "monkeypatch.setenv(\"FLASK_ENV\", env)", "if parent.left == self:", "tree = tree.insert(-16)", "import datetime", "m.ego_map_ops = m.vision_ops.fss_logits", "sh = reshape_conv_feat.get_shape().as_list()", "ctc_labels = [9, 9, 9, 1, 9, 2, 2, 3, 9, 9, 0, 0, 1, 9, 1, 9, 9, 9]", "attempt = self.left.ceil(label)", "if alive < 2:", "(\"other\", \"other\", False),", "flags.DEFINE_integer(\"train_steps\", 100000, \"Max train steps\")", "\"Minimum variance in posterior h0 codes.\")", "train_data_x[support],", "@pytest.mark.parametrize(\"mode\", (\"r\", \"rb\", \"rt\"))", "codecs.lookup(name)", "L, H = max(0.0, a2 + a1 - self._c), min(self._c, a2 + a1)", "batch_norm_param=batch_norm_param)", "if self.color == 1:", "other.bottom_root = self.bottom_root", "index_desc = 1", ") or (", "class Wrapper:", "attempts.append((loader, srcobj, rv))", "if isinstance(o, X):", "ax.scatter(", "gt_shape = tf.shape(m.readout_maps_gt)", "for args in failing:", "image_format = 'JPEG'", "d['feedback_factors_or_rates'] = flags.feedback_factors_or_rates", "\"Increase weight of l2 cost to avoid local minimum.\")", "IC_PRIOR_VAR_MAX = 0.1", "assert rv == '<a ng-data=\\'{\"x\": [\"foo\", \"bar\", \"baz\\\\u0027\"]}\\'></a>'", "with tf.name_scope('split'):", "next_node = i.parent.parent", "self.right._insert_repair()", "d['ic_prior_var_min'] = flags.ic_prior_var_min", "datasets = load_datasets(hps.data_dir, hps.data_filename_stem)", "all_occupancys, num_neurons=args.arch.rom_arch.num_neurons,", "a2_new += s * (a1_new - self._c)", "self._eps = 0.001", "self[key] = value", "'\"7\": \"foo\",',", "self._error[s] += (", "print_results(\"Rotating right and left\", test_rotations())", "if args.arch.readout_maps:", "hps.dataset_dims = {}", "This script converts the training and testing data into", "hps.num_steps = datasets.values()[0]['num_steps']", "\"Initial checkpoint (usually from a pre-trained BERT model).\")", "hps_for_saving = jsonify_dict(hps)", "lambda app: io.StringIO(\"Test\"),", "centroids = revise_centroids(data, k, cluster_assignment)", "flags.DEFINE_string(\"device\", DEVICE,", "a2_new += s * a1_new", "print(\"Property 4\")", "IC_ENC_DIM = 128", "tf.reshape(readout_maps, [-1, len(task_params.readout_maps_crop_sizes)]),", "Furthermore, if the data set type is training, it would contain one more field:", "view.view_class = cls", "class TestHelpers:", "\"/add\",", "self.mergeHeaps(newHeap)", "self._auto_norm = auto_norm", "assert False, (\"%s does not exist.\" % key)", "context,", "image/encoded: string containing JPEG encoded image in RGB colorspace", "\"Strength of KL weight on controller output KL penalty.\")", "return self.samples.shape[0]", "if self._is_unbound(i2):", "with app.open_resource(\"static/index.html\", mode) as f:", "title_re_match = link_re.match(raw_title)", "sys.stdout = open(os.devnull, \"w\")"]